	- Data Lake provides scalability to handle the data but it can have reliability issues.
	- This is where delta lake comes in as a solution.
	- It is open source storage layer that brings reliability to data lakes by providing data warehouse like  features on top of data lake.
	- How delta lake and various component in this ecosystem allows us to build a lake house architecture.
	
	
	
	Agenda:
	
	- Understanding delta Lake and its ecosystem and working.
	- Handling batch and streaming data seamlessly on delta lake.
	- Optimizing storage and queries using Delta Engine
	- Building end to end Lakehouse architecture
	- Automating ETL pipelines using Delta Live Tables
	- Implementing variety of common use cases
	
	
	
	
	Prerequisites:
	- Apache Spark 
	- Microsoft Azure
	
	
	
	Delta Lake has a big ecosystem! So terms like are like : 
	
	Delta Sharing
	Delta Engine
	Delta Cache
	Delta Format
	Unity Catalog
	Delta Live Tables
	Delta Table
  
  ![image](https://user-images.githubusercontent.com/24469318/208907969-254d3149-05b3-4858-ade1-d1a7406544f3.png)
  
**  What is ACID Guarantess on Transactions?**

	- In databases each transaction you perform provides ACID gurantees.
  A - Atomicity 

	- This means in a transaction either all the changes will written or no changes will be written 

For eg.

If there is a table and you are writing 2 records to that table in the transaction.
Either both the record will be stored or non of them will be stored.
Partial output will not be written. 
So transactions have atomic behavior.![image](https://user-images.githubusercontent.com/24469318/208908011-6447c2e8-9ef6-4fca-901d-4944570da251.png)


	Lakehouse
	Delta Architecture
	Delta-RS
	Delta-Standalone
	
	
	Overview:
	
	Understand what is Delta Lake and how it works
	Setup Azure Databricks environment
	Work with Delta Lake storage
	Handle streaming data on Delta Lake
	Optimize with Delta Engine in Databricks
	Build a Lakehouse architecture
	Build ETL pipelines with Delta Live Tables
	Implement common use cases
	
	
	
	
	
	Overview
	
	Understand the need for Delta Lake
	How Delta Lake works?
	ACID guarantees on Delta Lake
	Understand Delta ecosystem
	Setup Azure Databricks environment
	
	
	
	
	Need for delta lake:

What is data lake?

Data Lake is a cost effective  central repository to store all types of data at any scale, in the form of files. 
-can store Structured, semi-structured or unstructured data.
-Store data at any scale and it is in the form of files.
-Azure storage, amazon s3, on prem data center to create a data lake 


	- Spark works very well with data lakes.
	
	How Spark Writes to Data Lake?
	
	-  Let see we have a Spark cluster and a data lake 
	- Now you want to run a job to process sales data and write output in csv format to sales folder in data lake.
	- To do that spark will create multiple task to process data in parallel.
	- And these task will parallelly write the files in  data lake folder .
These are also known as partition files. So the final output are stored in these multiple files.

![image](https://user-images.githubusercontent.com/24469318/208907786-ed6c98f8-bd52-4753-b781-bd7860f26e07.png)


What is ACID Guarantess on Transactions?

	- In databases each transaction you perform provides ACID gurantees.
  A - Atomicity 

	- This means in a transaction either all the changes will written or no changes will be written 

For eg.

If there is a table and you are writing 2 records to that table in the transaction.
Either both the record will be stored or non of them will be stored.
Partial output will not be written. 
So transactions have atomic behavior.
![image](https://user-images.githubusercontent.com/24469318/208908287-b38079e9-4d98-44ba-aaca-690c381d03f9.png)

![image](https://user-images.githubusercontent.com/24469318/208908421-489367d4-1fd2-43bc-a2c6-28bea2b88e44.png)

![image](https://user-images.githubusercontent.com/24469318/208908447-1bdc5bf0-14d2-4f28-a6e4-2f361e0e789f.png)


C- Consistency
	- This means data will always remain in a valid state before and after the transaction.
	
	For eg. You are storing bank balances and you define a constraint that  balance cannot go below 0. So balance is 100 and you try to withdraw 200 the transaction will not succeed. 
This ensures the data remains consistent.

![image](https://user-images.githubusercontent.com/24469318/208908488-f2600a6c-b6eb-410c-9552-d190828167c5.png)

I - Isolation 
	- This means even if multiple transactions are running they shouldn't interfere with each other or affect data consistency.
	
Let's say a table has 3 records.

![image](https://user-images.githubusercontent.com/24469318/208908651-6379a805-a749-45cb-ac99-0ab3be73498a.png)

![image](https://user-images.githubusercontent.com/24469318/208908696-689f38ab-fee9-476b-a685-c329c0319f9c.png)

	
And the transaction is writing 2 more records.![image](https://user-images.githubusercontent.com/24469318/208908722-d02038bf-fa96-4639-8d84-643712a3cf4a.png)

![image](https://user-images.githubusercontent.com/24469318/208908746-fb371f82-928b-492c-a0f2-12a38c390b90.png)

One is written and another is about to get written.![image](https://user-images.githubusercontent.com/24469318/208908812-8fc6a40f-0897-4588-a508-bb2f2d788622.png)


![image](https://user-images.githubusercontent.com/24469318/208908856-0e039eb2-fe26-45b6-87c5-306551b3205e.png)


Now if reader is reading from the same table.  The read transaction will only see 3 records and not the new data until the write transaction is complete.![image](https://user-images.githubusercontent.com/24469318/208908878-0a145b6f-0d60-4475-acab-90aa3081dc0b.png)


![image](https://user-images.githubusercontent.com/24469318/208908895-04e70e73-031c-4593-bdf4-0603946a3c89.png)


This ensure that each transactions runs in isolation. 



Durability: 
This means once the data are committed and changes are stored they should persist even if any system failure occurs.![image](https://user-images.githubusercontent.com/24469318/208908920-3fe08b24-4545-48b5-ac11-a68e7123102d.png)


![image](https://user-images.githubusercontent.com/24469318/208908958-75a7e7c0-7c14-4bd9-8b6f-8f1e9c02e2cf.png)


So these ACID guarantees on database will make database a reliable storage platform .


 Even though Data Lake is cost effective, provides scalability, and stores all types of data, but it does not provide
 ACID guarantees. 

So let's see what problems can occur.
 It's very important to understand this. 
Let's say you have a folder in Data Lake to store sales data, and two partition files are
 already stored there, part file 1 and part file 2. 

![image](https://user-images.githubusercontent.com/24469318/208909015-8103d955-60b2-4f61-9b98-87c7ae728f16.png)

![image](https://user-images.githubusercontent.com/24469318/208909073-72458cb0-7bf2-4231-b30c-2a0d7e206034.png)


Let's see what happens if there is a job failure while appending the data.
 Say there is a writer process that wants to append two more partition files using two tasks. 

So it starts to write two files, and one file is written. Now let's say the job fails while processing the part file 4 with a runtime error like division by zero.

![image](https://user-images.githubusercontent.com/24469318/208909099-adbc1fe1-b1ed-4fc0-9295-1319cf60cb24.png)

![image](https://user-images.githubusercontent.com/24469318/208909140-b94b5087-7059-47a6-b803-6b726478941c.png)




Now carefully notice that folder contains partial data. On the other hand, say a reader wants to read the data in Sales folder, so it will read part files 1, 2, and 3, and this means it's reading partial or inconsistent data.
So a failed job breaks the atomicity guarantee and puts the data in inconsistent state. Make sense?![image](https://user-images.githubusercontent.com/24469318/208909172-509f6973-af79-4be7-a536-c41e9a37876a.png)




![image](https://user-images.githubusercontent.com/24469318/208909196-dd26b347-80ba-4869-9c3d-9a03727fe495.png)




 Let's take another example and see what happens if there is a failure while overriding the data. 
Once again, there are two part files present in the folder. Now a writer process wants to overwrite the existing data. 
In Spark, overwrite is a two‑step process. Delete the existing files and then write the new files. 

![image](https://user-images.githubusercontent.com/24469318/208909222-0fb4af27-c035-4d39-955f-f6aecfa7ff25.png)




![image](https://user-images.githubusercontent.com/24469318/208909245-f816207f-cc71-4f44-ba7d-7a3219af4382.png)





So first, writer deletes the existing files and then starts to write the two new files.![image](https://user-images.githubusercontent.com/24469318/208909330-3d98fc41-d702-46ed-8975-b3b4da521ac0.png)



![image](https://user-images.githubusercontent.com/24469318/208909353-dbdc364a-b3fe-4f70-a492-ff571fd5eca9.png)


 Once again, say one file is written and the second file is still getting processed. ![image](https://user-images.githubusercontent.com/24469318/208909382-56587ce8-8d5e-4212-a607-67947e3323e3.png)


![image](https://user-images.githubusercontent.com/24469318/208909395-afd93c9d-5440-415b-bf3b-8dc728841546.png)


At this point, the job fails with a runtime error, so the second file could not be written. ![image](https://user-images.githubusercontent.com/24469318/208909419-b6f83e27-72c2-4dd3-a950-edb4e47ec7f3.png)


![image](https://user-images.githubusercontent.com/24469318/208909428-12a3b6c5-ca4d-4b8f-9789-b2b5574601bd.png)

Now a reader process wants to read the folder, so it will only read the new part file 1.
This means it's reading inconsistent data and we have lost the previous data as well. ![image](https://user-images.githubusercontent.com/24469318/208909456-f8377d82-8a09-48b6-a753-2edd9899e149.png)



![image](https://user-images.githubusercontent.com/24469318/208909488-f615eef0-729b-49f4-8088-67293fc5db81.png)




So this failed job has broken atomicity and consistency, and durability is not provided because existing data is lost.



 All right, now let's see what happens during simultaneous read and write.
 Let's say a writer process wants to append two more partition files, part file 3 and part file 4.
 File 3 is written while file 4 is still getting processed.![image](https://user-images.githubusercontent.com/24469318/208909519-0b1447f2-a7ef-445f-a8d2-f46b1a062505.png)
 
 
 
 ![image](https://user-images.githubusercontent.com/24469318/208909574-4bb4ce96-e08a-4655-9340-12b39454006f.png)
 
 
 
  
Now at this point, let's say a reader process wants to read the data, so it reads the whole folder while writing is still in progress. 
Now carefully notice it is only going to read three files present in the folder. And once again, this means reader is reading inconsistent data.
This is where isolation is so important. Any data written by the writer should not 
be visible to the reader until it is committed. ![image](https://user-images.githubusercontent.com/24469318/208909618-94b40ad3-8389-40f0-8dde-ff285358120e.png)


![image](https://user-images.githubusercontent.com/24469318/208909675-56ef933c-ddcf-4e83-a667-4a6f20386026.png)






But since there is no mechanism in Data Lake to check this, this breaks the data consistency and isolation of transaction. 
And finally, let's see what happens if you add data with a new schema.


![image](https://user-images.githubusercontent.com/24469318/208909695-9213c6fb-5bd0-43ed-9ddc-a01702de3281.png)

![image](https://user-images.githubusercontent.com/24469318/208909743-2af2a617-ea38-4afb-83ae-dfa165d538fe.png)



Let's say the current dataset has three columns. Now the writer process wants to append two new files, but this time the output will have four columns, so it will write two files to the folder. The new output files have four columns, but no schema validation is performed before writing the data. And this is where the problem starts. When the reader process starts reading the folder, it does not know how many columns are there. If it reads file 1 or 2 first, 
it will consider three columns, but if it reads file 3 or 4, it will think that there are four columns. And this does not provide a consistent view of the data because there is no schema validation before writing. Make sense?
![image](https://user-images.githubusercontent.com/24469318/208909767-09fc300b-3401-40c3-ba79-4d2e9a98c3af.png)


![image](https://user-images.githubusercontent.com/24469318/208909789-925e7fbf-a239-4156-aa08-97b6f4e640a6.png)




 So as you have seen, there are several challenges associated with Data Lake.
	-  It can have data reliability issues. If there is a job failure while writing, it may lead to data corruption, and you cannot even roll back the changes. 
	- No data validation is performed while writing, so a column could be written as an integer in one transaction and a string in another. And there can be consistency issues while reading the data.
	-  Also, if you have streaming data in your projects, it can be tough to handle along with batch data. 
	- Now one of the biggest drawbacks is that you cannot perform updates and deletes on files easily. And because of this, implementing GDPR, CCPA, or other compliances becomes a challenge. 
	- Then there can be data quality issues. As you saw, schema is not verified before writing the data. And unlike databases, you cannot apply checks on data. You have to handle all these things in your code. 
	- There can be query performance issues on Data Lake, which we'll be discussing later.
	-  And it's also difficult to maintain historical versions of data. 
	- So because of this, storing data in Data Lake becomes challenging. 
	- This is where Delta Lake comes in and can help us solve these challenges.
 But the question is, is Delta Lake the only option? No. 
There are other projects in the same space, like Apache Iceberg and Apache Hudi that works similarly to Delta Lake. Sounds good?

Even though data Lake is cost effective, provides scalability , and stores all types of data but is does not provide ACID gurantees.



So let's see what problem can occur 
It's very important to understand this.

Data Lake does NOT provide ACID guarantees



Challenges with Data Lake

Data reliability issues
	- Data corruption because of failures - no rollback!
	- No data validation
	- Consistency issues while reading data
Handling Batch and Streaming data together is tough
NO updates / deletes / merge on files
	- Difficult to implement GDPR / COPA compliance
Data quality issues
	- Schema isn't verified before writing
	- Cannot apply checks on data
Query performance issues
Difficult to maintain historical versions of data




Delta Lake can help us solve these challenges!




But is Delta Lake the only option?
Apache Iceberg
Apache Hudi


How Does Delta Lake Work?
Now that you have seen why Delta Lake is required, let's see how it works.
 -  Delta Lake is an open‑source storage layer that brings reliability to data lakes.
	-  Think of this like a small utility that you need to install on your Spark cluster. 
	- All right, so first let's see how to save the data in Parquet format. If you're not aware about Parquet, basically, it's a columnar format for files. Let's say you have created a DataFrame in Spark. Now when you save this DataFrame in parquet format, it will create and store multiple partition files in Parquet format. 






![image](https://user-images.githubusercontent.com/24469318/208909890-5d2515ea-7f00-4247-a4c1-9360229d5c7a.png)


	- On the other hand, in case of Delta format, first, you need to deploy Delta Lake component on Spark cluster. Once again, you have a DataFrame. Now when you save the DataFrame in Delta format, it will again write the partition files, and these files will be in Parquet format. Until this point, it's exactly the same as previous case. But along with Parquet files, it also stores the transaction log, and this is where the magic starts.
![image](https://user-images.githubusercontent.com/24469318/208909958-183b9395-c33e-4466-b6b5-c813df864e2b.png)



![image](https://user-images.githubusercontent.com/24469318/208909993-8bdef010-f1cc-432c-9167-621cf1156275.png)



-  The transaction log contains the log of every single operation that has been performed on this data. All right? 
- But before we see what's in transaction log, let's see how you can write the data in Delta format. 
- On the dataframe, use the write property, define the format. 
- For Delta Lake, use Delta format just like you can also use other formats like CSV, Parquet, et cetera, and then use the save method. 
- This will store data in Parquet format along with transaction log. Simple, right? ![image](https://user-images.githubusercontent.com/24469318/208910015-5f57cf33-e15b-405a-af83-82df98a6db5a.png)



![image](https://user-images.githubusercontent.com/24469318/208910079-5649e419-8032-4d54-b829-e0d685cbf3cc.png)



- Let's dive in a bit deeper. Let's say you want to store customer data in Delta format. 
- Now inside the Customer folder, Spark will create a subfolder called _delta_log.
-  This is where the transaction log will be stored. 
- Now on first write operation, let's say it creates two part files, 000.parquet and 001.parquet
- But since you are storing the data in Delta format, Spark will also create a file in _delta_log folder called 000.json.
-  This is the first transaction entry.![image](https://user-images.githubusercontent.com/24469318/208910123-a93b662b-3f6c-4e67-a1d6-c5462d683d45.png)



![image](https://user-images.githubusercontent.com/24469318/208910235-a208b410-67b1-4fcd-a81e-74610a72f865.png)




	- 
 Now you want to insert more data. This writes a new part file in Customer folder and then adds one file in log folder, 001.json. This is the second transaction entry. 
 In the same way, during third write operation, one Parquet file is added and a log entry is created as well. 
 


![image](https://user-images.githubusercontent.com/24469318/208910305-9f00b5a3-b9e0-4567-876b-dfcbcd16dbb3.png)



This means for each write operation, the part files are written first, and then a transaction log file is added to _delta_log folder in JSON format. 
- And remember, only when the log file is written the transaction is considered complete, otherwise not. Make sense? ![image](https://user-images.githubusercontent.com/24469318/208910328-45668433-d58b-4949-8381-a4885ef0ad25.png)


![image](https://user-images.githubusercontent.com/24469318/208910376-e81d8bb5-891c-4b45-84a5-67cf1fe652c9.png)



	
	- Now let's see how read operation works.
-  For each read operation, first, transaction log files are read, and after that, partition files are read based on the information in log files. 
 This means it will first read all the log files, 0, 1, and 2, and then read all four part files based on the log information. ![image](https://user-images.githubusercontent.com/24469318/208910396-89cae718-7f06-4043-b5e3-bba6e18ea116.png)



![image](https://user-images.githubusercontent.com/24469318/208910441-074c41b3-55ec-42e8-8d1a-bdf9ce062d35.png)




	- 
	- All right, now that you have seen how read and write operations work, let's dive further into transaction log. 
	- Let's say you're writing the customer records for the first time.
	-  Spark has created two partition files for this. 
	- The dataset has two columns, Id and Name, and each file has two records. 
	- Then, as you know, 000.json file is added to the log folder, and then the transaction is complete. 
	- Now notice the log file contains information that part file 1 was added and part file 2 was added. Sound good? 
	- Now let's say you perform an insert operation, and this is going to append one more file with new data. 
	- First, part file 3 is added with two new records, and then 001.json file is added to the log.
	-  Once again, it mentions an add operation, and part file 3 is now part of data.
	-  And now let's see how an update operation is performed. Let's change the name from A to AA where Id is equal to 1. 
	- Currently, record for Id 1 is present in part 1. Now performing an update is very tricky.
	-  So what will happen here, Delta will create a part 4 file. It will copy all the unchanged rows from part 1 to part 4.
	-  This means record for Id 2 is copied as it is, and then it will add the modified row for Id 1. Sounds good?
	-  It will then write 002.json in transaction log. 
	- Now notice first it mentions that with the current transaction part 1 is no longer required. 
	- This is because all unchanged data from part 1 has been added to part 4 and modified rows have also been added to part 4. 
	- So remove part 1, and then add part 4. 
- This is how update operation is performed. Now let's see how to read the data.![image](https://user-images.githubusercontent.com/24469318/208910471-568f3460-da9d-4634-bc2c-126de34b7967.png)




![image](https://user-images.githubusercontent.com/24469318/208910512-1baad113-9d7f-4870-a35c-8d5df30df12e.png)


	-  First, it will read log files with 000.json being read first. 
	- So according to it, it collects the information to read part 1 and 2. 
	- Then according to 001.json, it needs to read part 3, but according to 001.json, part 1 is not required, so it will remove part 1 from reading list and then add part 4 to reading list.
	-  So this means to read latest data, you need to read part 2, 3, and 4.
	-  Now based on this, the final output is prepared. It contains records from part 2, from part 3, and part 4, which includes the modified row.
	-  So that's how transaction log of Delta Lake is used to read and write the data. Interesting, right? 
	- At a high level, let's see what features does Delta Lake provide. It provides ACID guarantees. 
	- So even if there are job failures, data is not corrupted. 
	- And it provides data consistency while reading the data. Then you can perform inserts, updates, and deletes on the data with significantly less overhead. It also enforces the schema that helps in preventing bad data being added. You can also look at previous versions of data using time travel. Next, it can seamlessly handle batch and streaming data. You can also apply various data quality checks, and it can significantly improve query performance using statistics. And there is much more. Keep a note of these points. We'll discuss them one by one. So to summarize, Delta Lake provides database‑like features on top of Data Lake.

ACID Guarantees on Delta Lake
Now let's see how Delta Lake provides ACID guarantees. Previously we discussed what are ACID guarantees on transactions. Let's quickly summarize that. A stands for atomicity. This means either all changes or no changes should be written for a transaction. C stands for consistency, so before and after a transaction, data should remain in a valid state. Then there is isolation. Each transaction should run in isolation and it should not be affected because of other transactions. And finally there is durability. This means once the transaction is committed, the data should persist even if there is a system failure. So let's see how Delta Lake provides these guarantees. Let's say you are storing the data in Delta format. Two Part Files have been stored and a log file is stored corresponding to that. Now let's see what happens if there is a job failure while appending the data. Let's say a writer wants to write two part files. One file is written while the other is still getting processed. Now at this point, the job fails with a runtime error. This means Part 4 could not be written, but also notice that no entry is made to transaction log. That's what makes a transaction atomic. Now when a reader wants to read the data, first it reads the log file, and since 000.json only points to Part Files 1 and 2, it only reads these two files. Carefully notice that even though the folder contains partial data in terms of Part File 3, it will ignore that file, and that's why the reader is able to read consistent data, so even in the case of job failure, the dataset will not be impacted. Sounds good? Now let's see what happens if there is a job failure while overwriting the data. Let's say a writer process wants to overwrite the existing files by providing two new files. As you saw previously, Spark deletes the existing files, but in the case of Delta format, existing files are not deleted. Now it will start writing two files. Once again, one file is written and there is a runtime failure. Now even though partial data is written, no entry is made to the transaction log. And I'm sure you can guess this one. If a reader wants to read data, it only reads Part Files 1 and 2 based on the log, right? So the reader is not just able to read consistent data, even the previous data is not lost. This is how Delta Lake provides the durability. And finally let's see how simultaneous read and write work. Once again, the writer wants to write two more files, so it starts writing the files. File 3 is written to the folder, while File 4 is still getting processed, but at the same time a reader process wants to read the data. So it reads the log file, 000.json, and based on the log, it only reads Part Files 1 and 2. This means the reader is not reading any partial or dirty data while writing is in progress. That's how the writer process is running in isolation. Also notice, once File 4 is ready, it will be added to the folder and then an entry will be made in the transaction log, so next time when a process wants to read the data, it will read two log files and then read four part files. So as you have seen, this is how Delta Lake provides ACID guarantees. Remember, Delta Lake uses optimistic concurrency and it uses WriteSerializable isolation level. You will learn more about it in the later modules.

Understanding Delta Ecosystem
Before we see Delta Lake in action, let's understand, at a high level, what do we have in the Delta ecosystem. There are a whole lot of tools and architectures being built around Delta Lake. First, there is Delta Lake storage, then there is Delta Engine, there is Delta Sharing, Delta Live Tables, Delta architecture, and Delta connectors. Let's discuss them one by one. Remember, this is just a high‑level introduction to different options in the ecosystem. We'll be discussing most of these options in detail. First, there is Delta Lake storage. This is what we have been discussing so far. It is an open‑source storage layer that runs on top of Data Lake. As you saw, it adds transaction log along with partition files, and this helps to bring database or data warehouse‑like features to data lake. Remember, Delta Lake storage is the core component, and all the other components depend on it. Then there is Delta Engine. Now this is only available in Databricks, and it has three features. First, it has a high performance, vectorized query engine called Photon. Then it also has a caching feature called Delta cache, and there are several built‑in optimizations. All these features can help to run your queries exponentially faster as compared to Spark. Sounds good? Then there is Delta Sharing. Now this is an open‑source protocol that allows us to share Delta Lake data. If you have your data stored in Amazon S3, Azure Storage, or Data Lake or in GCS, you can securely share them in real time, and once it is shared, it can be consumed by any engine. For example, data presented in Azure Storage can be securely consumed through Spark on AWS, or you can use other engines like Rust, Power BI, pandas, etc. Next, there are Delta connectors. There are various open‑source connectors to directly access Delta Lake storage. While Delta Sharing can be used to share data with external engines, but if you have direct access to Delta Lake storage, you can use these connectors. Now, there is a native Java library called Delta Standalone that can be used to read and write to Delta tables, and it is not dependent on Apache Spark. In the same way, there is a native Rust library, library to ingest data from Kafka, and other libraries for Hive, Presto, and Power BI, and it's a growing ecosystem. Interesting, right? Then there is Delta architecture. This architecture is built using Delta Lake storage. It can help improve the quality of data, and data can be served to various personas like business users, data scientists, ML engineers, etc. Also note that Delta architecture is part of a bigger Lakehouse architecture. And then there are Delta Live Tables. Just like Delta Engine, this is only available in Databricks. It's a framework that can be used to build data pipelines on top of Delta Lake storage. Now in these pipelines, you can perform transformations and even enforce data quality. So Delta Live Tables can be used to quickly build a Delta architecture. So you can see, there are a lot of entities available in the Delta ecosystem, and I have just introduced them. Take a note of them. We'll go through most of these options, one by one. Before we wrap this clip, notice that you can visit github.com/delta‑io to check the open‑source projects that we just discussed.

Setting up Azure Databricks Environment
Now to start doing hands‑on with Delta Lake, let's set up the Azure Databricks environment. Here is some important information. We'll be using files from New York City Taxi Service, and I have modified them for our demos. The course will have a mix of PySpark and SQL code. PySpark will be majorly used for reading and writing data, as well as streaming, and SQL will be majorly used for table‑related commands. But let's say if you're only interested in PySpark code, I have shared additional notebooks for it. Now to start with, you can download all the demo files, scripts, notebooks, etc, from the exercise files section of the course. Here you will see how to set up the environment using Azure Cloud Shell. But you can refer to Setup instructions document for an additional set of details. Now let's see the steps to set up the environment. We'll use Azure Cloud Shell to set up Azure Data Lake Gen2 account and then copy the account and the container names. Then, we'll upload files into Data Lake container. Next, we'll create an Azure Active Directory application and copy its client ID, client secret, and tenant ID. Now, once the Azure AD app is ready, we'll grant access to it on the Data Lake account. Followed by this, we'll create an Azure Databricks workspace, and finally, we'll mount Data Lake with Databricks workspace using Azure AD credentials copied in the previous step. So let's see that. First, go to Azure portal by using portal.azure.com. Here click on this button to open Azure Cloud Shell window. This is the PowerShell and CLI feature built into Azure portal. If you're using it for the first time, Azure will ask you to create a storage. You can create one. Make sure you're connected to PowerShell. First, let's upload all the PowerShell script files one by one. Click on Upload/Download files button, select the file, like 1‑ResourceGroup, and upload it. In the same way, upload all the PowerShell script files. Now let's start by creating a resource group. For that, let's create a variable $RGName and provide the name of resource group. Then, create a variable for location. Let me define the value as eastus2. Now let's run the script, 1‑ResourceGroup.ps1, and you can see resource group is now ready. Now let's create a Data Lake Gen2 account, create another variable, StorageName, and provide a globally unique name. Let me keep it as pltaxidatalake. Then run the script 2‑Storage.ps1, and this has created a new Data Lake account. If you see error, like storage name is not available, assign the value to variable again, and then run the script. Now let's upload some files, minimize the Cloud Shell window, and let's search for the Data Lake account. Now in the account, go to Containers from the left menu, and let's create a new container. Provide a name, say, taxidata, and create it. The container is now ready. Now open the container and create a directory called Raw, and lets upload certain files in this Raw folder. Create some subfolders and upload the files that you have downloaded from the exercise files. You set up instructions document to see the hierarchy of files. Back to the Cloud Shell, now let's create an Azure AD application. This will act like a service account. To do that, use the command Connect‑AzureAD, then define a variable for application name, let me keep it as ps‑databricksapp, and then run the script 3‑AzureADApp. Notice it has not just created an app, but it has also printed ClientID, ClientSecret, and TenantId. Store these values somewhere because this will be required for mounting. And now that the app is ready, let's grant it permissions to Data Lake account. Let's run another script, 4‑RoleAssignment. This will assign the role, Storage Blob Data Contributor, to Azure AD app on the Data Lake account. This means Azure AD app now has access to read and write the files from Data Lake. Sounds good? And finally, let's create an Azure Databricks workspace, create another variable, and define the name for the workspace. Now let's run the script 5‑DatabricksWorkspace. This will take a few minutes, and you can see Databricks workspace with premium SKU is now ready. Now from the search bar, search for the workspace, and open it. And there we have the Databricks instance. Now click on Launch Workspace. This opens up a new window. We are now inside Azure Databricks workspace. You can see there are different views available, one for Data Science and Engineering, one for Machine Learning, and one to work with SQL. Let's select the first one. Now in the Workspace tab, you can create notebooks and write the code. From the Data tab, you can work with databases and tables and browse the file storage. From the Jobs tab, you can schedule and run the jobs, and from the Compute tab, you can create and manage the clusters. So let's go to Compute tab. Now to create a new cluster, click on Create Cluster and fill up the properties. Provide a name, say, DemoCluster, select mode as Single Node, then select the Databricks runtime version. A Databricks runtime is a collection of libraries that will be installed on the cluster. Since versions are released very quickly, always try and use the latest version. Next, set the termination time to 30 minutes. This means if you don't run any code for 30 minutes, the cluster will auto‑terminate. And finally, let's select the cluster machine size. That's it. Let's create the cluster. And you can see the cluster is now ready. Now in the Workspace tab, right‑click, and let's create a new notebook. You can import the notebooks that you have downloaded, so let me provide a name, Mount Storage. Select Python as the default language, and select Cluster as DemoCluster. Let's create the notebook. Now let's write the code to mount the Data Lake Gen2 account as a file system in Databricks. Think of this like connecting to a network drive. In the config, replace the values carefully that you copied earlier. Add client ID, then add client secret. Next, add tenant ID. In the source, add the name of Data Lake account. For me, it is pltaxidatalake, and the name of container, taxidata. Notice the mount_point name, /mnt/datalake. You can keep any name here. Now to run the cell, select the drop‑down, and select Run Cell. Or you can use Shift + Enter to execute this. Let's run this, and the Data Lake is mounted to Databricks. Let's verify if mounting is successful by listing the files in Data Lake. For that, let's use %fs ls. This means list of files. And then pass the mount_point, /mnt/datalake and run this. There you can see all the files present in Data Lake. So mount_point will act like a shortcut to access Data Lake. Sounds good? And with that, our environment setup is now complete.

Summary
In this module, you saw that while data lakes are great, there are several challenges with them, like they don't provide ACID guarantees, there can be data quality and reliability issues, etc. And this is where Delta Lake comes in. It is an open source storage layer that brings reliability to data lakes and can help to build data warehouse‑like features on top of a data lake. Now when you store the data in Delta Lake, the data is actually stored as Parquet files, but along with that, it also stores the transaction log, and this helps to provide ACID guarantees on Delta Lake. Now if you want to write the data to Delta Lake, the partition files are written first and then an entry is made into transaction log. And in case of reads, the log is read first and then the partition files. And the great part is you can perform all DML operations on it like insert, update, delete, and merge. Then you saw that Delta Lake has a big ecosystem and it has various components like Delta Lake Storage, Delta Engine, Delta Sharing, Delta Live Tables, etc. We'll be going through many of them. And finally, you saw how to set up Azure Databricks environment. Now in the next module, let's look at how to work with Delta Lake Storage.

Working with Delta Lake Storage
Module Overview
Hey, everyone. Welcome to this module on working with Delta Lake storage. So first, we'll see how to store the data in Delta format and what goes into transaction log. Then we'll see what is Delta table in the different ways to create it. We'll then look into various options to add the data to the table and then perform the ML operations on top of it. Next, we'll see how Delta Lake handles a schema with schema enforcement and schema evolution features and then how it can maintain data quality using table constraints. Finally, we'll discuss a very interesting feature, the time travel. So let's get started.

Storing Data in Delta Format
So let's start by storing data in Delta format. Now there are two ways to do that. First, you can directly store it in Data Lake. In this case, the files are stored in Parquet format and the transaction log is stored as well. But then no metadata is registered with Hive metastore, which is available in Databricks. And to query it, you must read the data directly from Data Lake. Now you can also store the data as a Spark table. Since it is in Delta format, it is called as a Delta table. Once again the files are stored in Parquet format and in Data Lake only. Transaction log is also stored, but then the schema of the data is registered with Hive metastore. This means to query it, you can either read the data from Data Lake or use the table name to access the data. This makes it easier to work with the data. Let's see the first option. Back to Databricks workspace, let's start by reading data for yellow taxi rides. First, let's define a schema for the same in PySpark. Now since it's a SQL notebook, use %python magic command to write the code in Python. And just to reiterate, the SQL notebooks, corresponding Python notebooks, and the data files are available in the Exercise files section of the course. Now to read the schema, define the column names and their data types using StructType method. The RideId column is the unique column here, and there are other columns like VendorId, Pickup and DropTime, pickup and drop locations, CabNumber, as well as DriverLicenseNumber, PassengerCount and TripDistance, payment‑related information, and the different amounts being charged. Let's execute this, and this creates a schema variable. Now let's read the data in YellowTaxis file. For this, let's use Spark .read method, specify header is true, and in the schema method, pass in the schema variable that we created above. Since it's a CSV file, use csv method and pass in the file path and assign this to a DataFrame variable, yellowTaxisDF. Let's run this. And notice there are close to 10 million records in the file. Let's display the data in the DataFrame, and you can see the data from the file based on the schema we defined. Now let's write the output of this DataFrame directly to Data Lake in Parquet format. On the yellowTaxisDF, use the write method. Let's specify mode as overwrite. This will overwrite any existing files. Let's partition it by VendorId. You can even create partitions on multiple columns. Provide the format as parquet and specify the path, which is YellowTaxis.parquet. Let's run this. Now before we look at the Parquet data, let's save this in Delta format as well. The command remains exactly the same, but this time let's use the format as delta, and the output folder name will be YellowTaxis.delta. Let's execute this, and the data is now saved. Now let's switch over to Data Lake in Azure portal go to Containers, and let's open taxidata container. And in the Output folder, you can see two folders, one for Parquet and one for Delta. Let's look at Parquet output first. Notice there are separate folders for each VendorId because of partitioning. Let's check data for VendorId‑2, and you can see multiple partition files have been created. In the same way, you can check for other partitions as well. Now let's check the Delta output. Once again, you can see different folders for VendorIds. Let's open folder for VendorId‑2, and notice there are partition files. This is exactly the same output as you saw in Parquet, right? But at the root folder, I'm sure you have noticed the presence of folder _delta_log. Let's open this. Since there is only one transaction, notice the presence of one JSON file. 0.json. Let's open this file. You can see commitInfo, there is metaData, and there are add file operations. Since there is a lot of information here, let me show you a sample log entry. I have only kept limited information here. Now, as you saw, there is commit information. This contains time at which commit was made and who made the commit, then what operation was performed. Since we wrote the data, the operation type is WRITE, and then there are operation parameters like mode as Override. It also contains operationMetrics, like how many files, bytes, and output rows are written. Then, it also stores metadata. This includes a schema like column name, its data type, if it's nullable, and other properties. Sounds good? And finally, it contains file operations, like which files are added, removed, et cetera, and the path of the files is stored. So you can see a lot of information is stored in each transaction log.

Creating Delta Table
Now let's see how to create a Delta table. Back to Databricks workspace, let's see different options to create a Delta table. First, let's create a database called TaxisDB. Let's use the command CREATE DATABASE IF NOT EXISTS and pass in the name. Let's run this. Now from the left‑side menu, go to Data tab, and there you can see the database taxisdb. Now the first option is to create a table by referencing the data stored in data lake. To create a table on Parquet data use CREATE TABLE command. There are other options like CREATE OR REPLACE TABLE and CREATE TABLE IF NOT EXIST. Then let's specify the name as TaxisDB.YellowTaxisParquet. Provide the format as PARQUET, and specify the location where Parquet data is stored. Since the data is already partitioned, there is no need to specify a partition clause. Let's execute this, and see how quick this is. This is because it's only registering the metadata in Hive Metastore. Once again, open the Data tab, and there you can see the table. In the same way, let's create table for Delta format. Let's keep the name as TaxisDB.YellowTaxis, specify format as DELTA, provide the path, and execute this. Now in the Data tab, both the tables are showing up. Click on the Delta table, and you can see the details, like creation and modification timestamps, partition column, total number of files, and the size of those files. It also shows the table schema and some sample data. Now from the History tab, you can also see the table transactions. We'll look at it in detail in just a minute. And now that the table is created, you can run typical queries on top of it. Let's run a SELECT COUNT query, and you can see there are 10 million records in the table. Now let's see some commands to check table details. There are various commands available. You can use DESCRIBE TABLE EXTENDED or DESCRIBE TABLE FORMATTED command and pass in the table name. Let's run this. Notice it's showing all column details. It is partitioned by Vendor Id and other details like storage location of data, Provider, which is delta, etc. Sounds good? Then there is DESCRIBE TABLE command and DESCRIBE DETAIL command, providing similar information. I would recommend to check these out. Remember, all these commands can run on Parquet tables as well. And finally, there is DESCRIBE HISTORY command to check the audit log. This shows the transaction log of Delta tables and only works for Delta tables. Let's run this, and you can see currently there is only one transaction done on the table, which is a WRITE operation. It shows some of the information from the transaction log like version number and timestamp, very important; user and the operation information; and operationMetrics, number of files, output rows, and output bytes. That's why this is a very useful command. Alright, now let's come back to table creation and see the second option. Let's drop the Delta table first by using DROP TABLE command. Remember, this does not remove the files, so let's remove the files by using dbutils.fs.rm command. Provide the path, and this will delete the files from datalake. Now let's see how to write a DataFrame as a Delta table. The command is very similar to what you saw before. On the DataFrame, use the write method, specify mode as overwrite, partition it by VendorId, and then provide the format as delta. Now, in the option, define the path in datalake where you want to store the files. Finally, use saveAsTable command and pass in the table name. That's it. Let's run this. So this not just saved the files in data lake, but also registers the table. Sounds good? Once again, let's run HISTORY command to check the transaction log. In the previous case, the operation was a WRITE operation, but here the operation is CREATE OR REPLACE TABLE AS SELECT, and that's the only difference. Now let's rerun the command to save as Delta table. Remember, this is using the overwrite operation. Let's run this. Now will it actually replace the existing files? The answer is no. Let's see why. Back to Azure portal, let's open the folder YellowTaxis.delta and go to the folder for VendorId=2. Notice the files from the first transaction and the files from the second transaction. Now if you remember from previous module, in case of an overwrite operation on Delta table, the existing part files are not deleted. It first adds new part files and then adds an entry into transaction log, so if an overwrite operation is running and someone tries to read the data, the read will still succeed. This helps to provide consistency in read operations. Makes sense? Let's run DESCRIBE HISTORY command again and see that there are two transactions. The first transaction wrote 23 files and the second one also wrote 23 files, but the latest transaction only refers to the newer 23 files. Finally, let's see the last option, which is to use a typical DDL command CREATE TABLE. Once again, let's drop the table first and then remove the files from datalake. Now let's use CREATE TABLE command to define the table. Provide the column names and their data types. You can also provide a comment for each column. Then specify type as DELTA. This is the default type in Databricks. If you don't add this statement, it will still be of DELTA type. Provide the location to store the files and the column for partitioning. Just like columns, you can also add a comment for the table. And here is an interesting thing, you can also define the generated columns. You don't need to insert data in these columns. They'll be calculated for each row that is inserted. To do that, define the column name and type, then mention GENERATED ALWAYS AS and provide an expression. So here, let's create three columns, PickupYear, PickupMonth, and PickupDay. Use YEAR function to get year component of PickupTime and use MONTH and DAY functions for the other two columns. Interesting, right? Let's execute this. And this creates an empty table. Remember, creating or altering table is also a transaction. As an exercise, run DESCRIBE HISTORY command to check that. Now let's run the command DESCRIBE TABLE EXTENDED. Notice the comment against a column and the comment for the table. So you can see, a Delta table can be created and described in multiple ways.

Inserting Data to Delta Table
Now that we have created the Delta table, let's see how to add data to it. Let's see different options to insert data to a table. You can do that by using INSERT command in SQL. You can also create a DataFrame in PySpark or Scala and then append it to Delta table. Or you can do that by using COPY command and SQL. Let's see these options one by one. Back to the Databricks workspace. We already have an empty table, YellowTaxis. Let's add data to it. First, let's use the INSERT command. This is a typical SQL command. Use INSERT INTO and provide the table name, specify column names, and then provide the values. Notice this record belongs to VendorID 3. Let's run this, and this inserts one record. Let's use a SELECT query to verify this and execute this. There you can see one record in the table. During development, it's very important to check the transaction log at each step, so let's use DESCRIBE HISTRORY command again. Notice it has created a new version and performed the WRITE operation. Now notice the operationMetrics. It has added one partition file and one row has been inserted, right? Now let's see the second option, appending a DataFrame to Delta table. So first, let's read a file from Data Lake called YellowTaxis_append and display the results. Let's execute this. So this file basically contains three records where where VendorId is 3. Now let's use this yellowTaxisAppendDF. Let's append this DataFrame to Delta location. On the DataFrame, use write operation. Now define the mode as append, and then define partitionBy clause. Specify format as delta and provide the delta folder path. Let's execute this. And this should append data to the delta folder. To verify, let's check the data in the table. Let's run the SELECT command, and there you can see three new records in the table. Now let's check the transaction log. Let's run this. Notice that using INSERT command or appending a DataFrame, both are WRITE operations with mode as Append. And see the operationMetrics? One new file has been added with three records. And now we have two partition files holding four records. Sounds good? All right, now let's see the third option, the COPY command. This is very interesting and has a lot of options. This allows to load the data from a file into Delta table. Let's see how this works. First, use COPY INTO command and provide the table name. Then, use the FROM clause and pass in the file or folder location. If you specify a folder path, it will pick up all the files, but you can restrict it to a few files by using FILES clause and passing the file names. Here, let's specify YellowTaxis.csv file. Next, define the format of the file. It supports various file formats like CSV, JSON, Parquet, Avro, ORC, text, and binary formats. And since we have a CSV file, let's specify the format as CSV. Then, COPY command also supports a VALIDATE clause. This will only check to see if file schema matches with the table and data can be loaded or not. But remember, it will actually not load the data. Now V ALIDATE ALL will check for all the rows, but you can also limit the records for validation by using this option. And finally, you can specify various format options. There are a lot of options available for each file format to handle schema, errors, etc. So let's add a CSV‑specific option, header, and set it to true. Let's execute this. And this throws an error because of VALIDATE clause. And what's the error? Incompatible data types for RideId. Why? This is because RideId is of type integer in the table, and since we are reading a CSV file, all data types are considered of string type. Make sense? Now, to resolve this, let's define a SELECT query with ride data types. Let's add a SELECT query within the FROM clause. Notice how we are referencing the file directly and then selecting the columns. To define the data types for the columns, use double colon followed by the data type, like RideId::Int. I have removed the VALIDATE clause as well. Now let's run the query, and notice this has loaded close to 10 million records in the table. Let's check the log. You can see the COPY INTO operation. And this has created 23 partition files. Now let's run the same COPY command again. So, what do you expect? This should double the data, right? Let's run this. And see, no rows have been inserted. This is because COPY command keeps track of all the loaded files, and if you try to reload the file, it will simply ignore it. This is really helpful so that no file is loaded twice. And what about transaction log? Let's check that. And you can see there is another COPY operation added to the log, but number of rows loaded is 0. Now there are times when you would want to reload the file. So to do that, let's use the same query, but along with that, let's add COPY options and set the force parameter to true. This will load the file again, even if it has been loaded before. I'm not going to run this command for now since it will duplicate the data in the table. So you can see there are different options to add the data with Delta table.

Performing DML Operations: Update / Delete / Merge
One of the great features of Delta Lake is that it allows to perform all the ML operations on data. So let's see how to run Update, Delete, and Merge on Delta tables. First, let's recap how these operations work. Let's say you want to store customer data. As part of first operation, you are writing two‑part files. The dataset has two columns, Id and Name, and each file has two records. Then, 000.json file is added to the log folder, and it stores that Part files 1 and 2 were added. Now let's perform an update to change the name from A to AA where Id = 1. Currently, record for Id 1 is present in Part 1. Now during an Update operation, a new part file is added. First, all unchanged rows are copied from Part 1 to Part 3, so record with Id 2 is copied as it is. And then it adds a modified row for Id 1. Next, it adds an entry into transaction log. Now notice with the current transaction, Part 1 is no longer required since all unchanged data from Part 1, as well as modified data, has been added to Part 3. So remove Part 1 and then add Part 3. This is how Update operation is performed. Now let's delete the record with Id = 3. This record currently exists in Part file 2, so a new Part file 4 is added. Once again, the unchanged rows are copied from Part 2 to Part 4, and the record supposed to be deleted is ignored. Finally, a log file is created. Since Part 2 data has been copied, so remove Part 2 and add Part 4. Make sense? Then there is MERGE command. A MERGE command can perform multiple inserts, updates, and deletes all in a single transaction. Let's see how these operations work. Back to Databricks workspace, let's check the passenger count for a RideId, which is 3 short of 10 million. Let's run this. VendorId is 3 and the PassengerCount is 1. Let's update this PassengerCount value. But first, let's see all records for VendorId 3. Here the INPUT_FILE_NAME function will return the file in which the record exists. Let's run this. Notice there are 2 files that contains VendorId3 data. One file contains only 1 record, and 2nd file has 3 records. Now the RideId that we want to update is present in the 2nd file with 3 records. Now let's run the Update statement. Let's set the PassengerCount to 2 for the RideId. Let's execute this. And you can see one record is affected. Let's verify if the record has been updated successfully, and you can see PassengerCount has been set to 2. Now let's check the Data Lake folder through Azure portal. Let's open VendorId=3 folder. Previously, there were two files, but after the update, one more part file has been added. Now let's open _delta_log folder and open the JSON file for the latest transaction. There you can see one file has been removed, which had the original record, and the new file has been added. This new file will now have unchanged rows and the updated row. Right? You can also check the commitInfo here, or you can check it using DESCRIBE command. So let's run the DESCRIBE command. Notice the operation is of type UPDATE. Now in the operationMetrics, see that number of files removed is 1, number of added files is 1, 2 unchanged rows were copied, and 1 row was updated. Make sense? As you saw in the diagram, DELETE operation is very similar. Let's see that. First, let's check if table contains a record with Id 1 short of 10 million. Let's run the query. And the record exists. Now let's run the DELETE statement. Use DELETE from table, and specify the WHERE clause. Let's execute this. And this affects one row. You can verify this by running the same select query again. And the query did not return any result. Let's check the transaction log, and there you can see there is a DELETE operation entry. It even shows a predicate that was used in the query. Now notice the metrics. One file was removed. A new file has been added. This means unchanged rows were copied from the old file to the new one. Two unchanged rows were copied, and one row was deleted. As an exercise, go to the Azure portal and try verifying this. So this is how DELETE works. Finally, let's see how much command works. First, let's read a set of records from a file which is YellowTaxis_changes and create a new data frame, yellowTaxiChangesDF. Let's run this. And you can see there are four records in this file. The first three records are already present in the table, and the last one is not. Now, this data frame is created in Python. To use it in SQL, let's use a method createOrReplaceTempView. This will create a view in SQL which will refer to this data frame. Think of this like a pointer to the data frame. So on the yellowTaxiChangesDF, use the method and pass in a name, YellowTaxiChanges, so the data frame can be referred in SQL with the name YellowTaxiChanges. Let's run this, and this creates a view. Let's run a query on this view, and there you have four records. So we are now querying the data frame. Now let's define a MERGE statement with MERGE INTO command, specify the table name that you want to update, which is TaxisDB.YellowTaxis, and give it an alias, tgt. Then in the using clause, specify the source dataset. In this case, it is the view YellowTaxiChanges. And let's give it an alias, src, and then define the join conditions between source and target datasets. Now I'm sure you have noticed that RideId is primary key, but why is there a join on VendorId? This is because data is partitioned by VendorId, so adding this to a condition will speed up the operation. Sounds good? Next, define the action when RideId matches between source and target. When matched, let's update the record in target table by taking values from the source dataset. For now, let's only update payment type. But of course, you can update multiple columns separated by commas. If you want to update all the columns, just use UPDATE SET *. Now in the same statement, define the action when record exists in the source, but does not exist in the target. This is defined by using the clause WHEN NOT MATCHED. Notice you can even add additional checks, like PickupTime should be greater than or equal to 2022‑03‑01. And then provide the INSERT statement as an action. You can even use INSERT *. Sounds good? Let's execute this. So a total of four records were affected in the Yellow Taxis table, three records were updated, and one record was inserted. Now that you know how to check transaction log, used DESCRIBE history command, you can check out operation, its parameters, and its metrics. So that's how you can run updates and deletes on Delta table, and even merge data between two datasets.

Schema Enforcement and Evolution
With changing requirements, the structure of the data continues to evolve, but it's also important that data remains of high quality. Delta Lake ensures this with schema enforcement and schema evolution features. Let's take an example. Let's say you are storing customer data in Delta format. There are two part files, and each file has three columns. Also, there is a log entry 0.json. As you have seen, the schema is also stored in the transaction log, so it stores three columns. Now let's see what happens if you try to append data, but with new schema. So writer process wants to write two more part files, but this time, the files contain four columns. This can happen if new changes are implemented or there is a bug in the code. So the partition files are written, but before an entry is made into transaction log, the schema validation is performed, and since the schema of new dataset does not match with Delta table schema, no log file is added to the folder. Now, if a reader wants to read the data, it will only read part files 1 and 2 since 0.json refers to only these files. This means the reader will seek consistent data with three columns only. So that's how Delta Lake enforces the schema. So before committing data, Data Lake checks the schema of new data against the delta table schema. Now a schema mismatch could occur if source contains additional columns or if the column data types do not match between source and delta table. Also notice that column names are case insensitive, so two columns with same name, but different cases cannot be added to the table. So what happens if there is a schema mismatch? First, no entries added to the transaction log. The job throws an exception, but as you saw, the new partition files still remain there in the folder. But there are times when you would like to change the schema of table based on source data. This is where Delta Lake provides schema evolution. With this, Delta Lake allows adding more columns to table while writing the data, but you have to explicitly mention that you want to make these changes. In fact, even the merge command supports schema evolution. Let's see how this works. Back to Databricks workspace. To see schema enforcement, let's start by reading a file, RateCodes1.csv in a data frame rateCodes1DF and infer the schema as well. Let's run this. Now this file contains three rows and two columns, RateCodeID and RateCode. Let's save this data frame in parquet format to Data Lake. Notice the mode is overwrite, the format is parquet, and the location is RateCodes.parquet. Let's execute this. Now let's save this data frame in delta format as well. As you know, the command is exactly the same, except the format will be delta, right? Let's run this, and this saves the data in delta format. Now let's try to append some records to both parquet and delta locations. Let's read another file, RateCodes2.csv in rateCodes2DF data frame. Carefully notice this file has three columns. There is an additional column called isApproved and it has three more records. So let's try to append data in rateCodes2DF to parquet location, and notice, we are using append mode. And there is a problem. The files with new schema have been appended. Let's read the data from this parquet location. Let's run this query and you can see it's only showing two columns. In fact, it may show three columns as well. This is because if older file is read first, it will show two columns, but if it reads new file first, it will show three columns, and this can cause reliability issues, right? Now let's try to append the same data in delta format. Notice the use of append mode in delta format. Let's run this, and there you can see the error schema mismatch detected. This is how schema enforcement works. Sounds good, but as we discussed, there are times when we need to merge the schema. It's very simple to do that. Use the same append command and add the option, mergeSchema is true. This will add new columns to Delta table. Let's execute this, and this works successfully. Let's verify the data by using spark.read.load command, and the format is delta. Let's run this. Here, you can see the data with a new column and isApproved column value for older rows is set to null. So that's how Delta Lake provides schema enforcement and allows to perform schema evolution.

Applying Table Constraints
Another way of maintaining data quality is by applying table constraints. Let's see how you can apply them on a Delta table. There are two types of constraints that are supported. First, there are NOT NULL constraints. This prevents a column from having null values. And second, there are CHECK constraints. Here, you can define conditions on the table, and these conditions are enforced on the data inserted in the table. Let's see how they work. Back to Databricks workspace. Let's first see the NOT NULL constraint. You can apply this to a column during table creation, or you can alter the column of a table to apply it. Here let's alter the table, YellowTaxis, and then alter the column RideId and set it to NOT NULL. Let's run this. And this creates a constraint. Remember, the constraint creation would have failed if RidId had it in null records. Let's check the transaction log, and you can see this is a CHANGE COLUMN operation which has also been logged. Let's verify the constraint. Notice I'm trying to pass NULL value for RideId column. Let's run this. And it throws an exception, InvariantViolationException, NOT NULL constraint violated for column: RideId. All right, now let's see the CHECK constraint. A CHECK constraint helps to enforce certain conditions on the Delta table. So let's alter the YellowTaxis table and add a constraint called PassengerCountCheck. Use the CHECK keyword, and then define the expression. Here, let's say the PassengerCount should be less than or equal to 5. Let's execute this. And notice it throws an exception. Since table already has records with more than 5 PassengerCount, this constraint cannot be applied. Make sense? As an example, let's change the expression and set it to 9. You can even add multiple conditions here. Like count should be less than or equal to 9, or it should be null. Let's execute this. And this successfully creates a constraint. Let's verify this by trying to insert a record with PassengerCount as null and execute this. And you can see it threw an exception because of constraint violation. Now, if you want to drop a constraint, you can use DROP CONSTRAINT clause and provide the name. Let's run this. And this has now removed the constraint from the table. And finally, let's check the log using DESCRIBE HISTORY command. And notice the two operations, ADD CONSTRAINT, and DROP CONSTRAINT. In fact, it even shows the expression. So that's how you can use constraints to maintain data quality of your Delta tables.

Accessing Data with Time Travel
One of the most interesting features of Delta Lake is time travel. Let's see how to access the data using time travel. Let's take an example, say you want to store customer data in delta format. The first transaction two files have been written in a log file as well. Let's say this transaction happened at 10 AM. Then second transaction happened after 10 minutes at 10:10, and 1 file was appended. Finally, the last transaction happened at 10:20 AM and another file was added. Now let's run a query SELECT * FROM Customers. This will go ahead and read the current version. This means it will read all the transaction log entries, and correspondingly, it will read all the part files, right? Now let's change the query. With the table name, let's add the version number we want to read. So let's use SELECT * FROM Customers VERSION AS OF 1. This means let's read the snapshot of data which was committed with version 1. So it will only read the log entries in version 1, and because of this, it will read part files 0, 1, and 2. This is what is known as time travel. So you can even read a previous version of the data, even if it has been updated or deleted. Interesting, right? And not just by the version, you can also read the data snapshot using timestamp. Let's change the query SELECT * from customers timestamp as of 10:05 AM. This means let's get the data snapshot at 10:05. Now at 10:05. The latest version was version 0 so it will only read log entry 0 and extract part files 0 and 1 corresponding to that. So time travel allows to access and even restore the previous snapshot of data, even if it has been modified or deleted, and you can access it either using version number or timestamp. This can be extremely helpful if data has been accidentally modified or when you want to compare the latest data with previous versions. Now remember, by default, the logs are retained for 30 days only so you can only access snapshots of the last 30 days, but this setting can be changed as well. Now how the previous logs are removed, we'll discuss it later when we talk about check pointing. So let's see how this works. Back to Databricks workspace. Let's first check the passenger count for a RideId, and you can see the PassengerCount is 0, so let's fix it. Now let's run an UPDATE command and set the PassengerCount to 1 for the same RideId. So let's execute this, and now the PassengerCount has been updated from 0 to 1. Let's check the table history, and you can see an UPDATE operation has been performed. Notice the latest version number. Now let's say if this record was present at version 0, so let's run the same SELECT query, but along with table name, let's specify the version number 0 and run this. No result. So this record was not added during initial load. Makes sense? So let's modify the query and check the value of PassengerCount at a prior version. Let's execute this, and it's showing PassengerCount as 0. Remember, the value in latest version is 1. So that's how you can check table history and perform time travel using version number, and as we discussed, it can also be done using timestamp. So in the query, use the clause TIMESTAMP as of and provide the timestamp. Let's run this, and you can see count at 0. Remember that timestamp need not be exact when the transaction happened. At any given timestamp, whatever is the last commit that will be considered. Now that you have seen how to query the previous version, let's see how to restore that version. It's very simple. Use the RESTORE TABLE command, provide the table name, and provide the version number which you want to restore. Let's run this. It will take a bit of time, but you can see the number of files after restore, the number of removed, and the number of restored files, and that's it. Let's check the log again. Notice the operation as RESTORE and even the version number which has been restored. Now let's run the same SELECT query, and this shows PassengerCount as 0, so the updated value of 1 has been overwritten. Sounds good. So time travel feature is extremely useful to protect and restore previous versions of data.

Summary
In this module, you first saw how to store the data in Delta format. Data can directly be stored in Data Lake or as a Spark Table. The Spark Table is called as Data table. You also saw that each log entry contains commit information, schema, file operations, etc. Then Delta table can be created in multiple ways. If you have already stored the data in Delta format, you can refer to it to create a table. You can also save the DataFrame as a table or use typical DDL command to do that. Then you work with various options to add data to the table. You can append the DataFrame using Python or Scala, or you can use INSERT and COPY commands in SQL. COPY command is very powerful, and even keeps a track of files it has loaded. You also saw how to run DML commands, like Update, Delete, and Merge, and how it is handled in Delta Lake. Then, Delta Lake also provides schema enforcement feature, and only the data with the right schema can be inserted. But if you want to change or evolve the schema, you can use the MERGE schema property. You can also maintain data quality by applying NOT NULL and CHECK constraints on the table. And finally, any data that has been modified or deleted can be accessed using time travel. This can be done using version number or timestamp. In fact you can even restore the table to a previous state. So that's how we can handle data on Delta Lake. In the next module, let's see how to handle streaming data on Delta Lake.

Handling Streaming Data on Delta Lake
Module Overview
Hey, everyone! Welcome to this module on Handling Streaming Data on Delta Lake. So here, we'll first understand how streaming is supported on Delta Lake. Then, we'll set up the streaming environment and build an end‑to‑end streaming pipeline to insert the data to Delta Table. Next, we'll also see how Delta Table can be updated with streaming data. Followed by this, we'll see a concept called transaction log checkpointing and how it works. And finally, we'll discuss some very interesting scenarios to handle concurrency on Delta Lake.

Understanding Streaming on Delta Lake
Let's start by understanding how you can process streaming data on Delta Lake. Let's take a scenario. Let's say you're tasked with the responsibility for processing New York City's Green Taxis data. Each cab has an app that generates events when ride starts and when ride ends. These events are stored in a stream ingestion service, like Kafka or Azure Event Hubs. From here, a stream processing service, like Spark Structured Streaming, picks up the data and stores it into Delta Lake. So when the ride starts, an event is sent to Event Hub. Let's say the event contains RideId of 1 and PickUpTime of 10:00 AM. Now Spark Structured Streaming picks it up and stores it into Delta table. Notice, since ride has just started, DropTime is set to NULL. Now, when the ride ends, another event is generated and sent to Event Hub. The end event also contains RideId, and it contains the DropTime, which is 10:20 AM. And based on RideId, DropTime is then updated in the Delta table. So Delta Lake not just allows to store streaming data seamlessly, you can also perform upsert operations. Sounds good? Let's change the scenario a bit. Let's say there are cabs from multiple vendors which are sending events to an Event Hub. Now, Spark job 1 will process data for only VendorId1, and Spark job 2 will process data for the rest of the vendors. But both these jobs will parallelly write the output in the same Delta Lake table. In fact, along with this, multiple readers can read the data from the table at the same time, and these readers and writers can be batch or streaming processes. So, Delta Lake is well‑integrated with Spark Structured Streaming. You can store batch, as well as streaming data together in a Delta table. You can perform streaming inserts, updates, and deletes there, and even load the data incrementally. Then multiple streams writers can write to the same table, and Delta Lake can seamlessly handle the concurrency. And finally, Delta tables can be used as a streaming source and a streaming sink. We'll be practically looking into all these aspects in this module. Now in this module, we'll be focusing on storing and retrieving data from Delta Lake using Spark Structured Streaming. Of course, we'll build an end‑to‑end streaming pipeline, but if you want to go deeper into Spark Streaming, you can check out my other courses on Pluralsight. For basics, you can check out Conceptualizing the Processing Model for Azure Databricks Service, and for advanced streaming, there is Handling Streaming Data with Azure Databricks. So let's start building streaming pipelines.

Setting up Streaming Environment
Let's start by setting up the streaming environment. There are multiple components we need to set up. Notice, all the scripts, commands, sample application, and Databricks notebooks that you will see here are available in the Exercise files section of the course, so you can download them and follow along. Back to the scenario. First, let's configure Azure Event Hubs as a streaming source. To do that, we'll first create an Event Hub namespace. This is a collection of multiple Event Hubs. Second, we'll create an event hub inside the namespace configured in the previous step. This is where we'll send the events. And third, we'll create two consumer groups inside the event hub. Two consumer groups will allow two processes to read the same data from event hub. Finally, we'll copy the namespace connection string. Back to Azure portal. Let's use the Cloud Shell once again and maximize the window. Make sure you're connected to the PowerShell terminal. Now let's define some variables first, one for ResourceGroupName, add the name of your resource group, for EventHubNamespace, provide a name for your namespace, for Location, and for EventHubName. Run these commands one by one. Now let's run the script 6‑EventHub.ps1. It may take a bit of time. In case you get an error that name for namespace is not available, define the variable again with a new name and then run the script again. This script will create the namespace and event hub in it and two consumer groups. And it's complete now. Now copy the NamespaceConnectionString and the name of event hub. We'll be using it shortly. Now let's configure a sample application that will simulate the taxis. For this, you will need to install .NET 6 on your machine. It can work on Windows, Linux, and macOS. The link for that is available in the instructions document of the course. Then, we'll need to configure the application by adding namespace connection string to it and the event hub name. Finally, we'll run the application from a terminal. Now open the folder where you have downloaded the application. The sample data is present in the Data folder. It contains rides for one day and has more than 250K records. Now open appsettings.json file in any editor. Here, base the connection string of namespace and the name of event hub. Let's save this. Now open the Command Prompt, I'm using it on a Windows machine, and go to the application folder. Here, run the application NycTaxiTelemetryApp. Now press Enter to start sending the events. Notice multiple events are being sent every second to Event Hub. These correspond to taxi rides. If you don't see an error, this means events are going successfully. P here signifies Pickup Event, or the start of a ride, and D means Drop Event when the ride ends. So multiple pickups and drops are happening every second. For now, let's stop the application. Now let's configure Event Hub in Databricks. For that, we'll need to deploy Event Hubs connector on Spark cluster. We'll do that using Databricks libraries. Once it is deployed, we'll configure Databricks notebook to use Event Hubs connector. There, we'll add namespace connection string and the name of event hub. Back to Databricks workspace. To install the library, go to the Clusters tab and click on the cluster to open it. Navigate to Libraries. From here, you can manage the libraries on the cluster. Click on Install new, that event hubs library is available on Maven. Go to Maven tab. To find the library, click on Search Packages. Now, select Maven Central from the drop‑down and search for azure‑eventhubs‑spark. There you can see the latest version. Notice the Group Id and the latest Artifact Id. Choose the latest release and then click Select. Once selected, click on Install. This will install library on the cluster, and it will be available to all the attached notebooks. That's it. The library's now deployed. And finally, let's configure the notebook to use Event Hubs. I'm using a new notebook with Python's default language. Here, let's add all the event hubs configuration in a new cell. Let's create a variable namespaceConnectionString and paste the connection string that we copied earlier. Add another variable eventHubName, and add the name which is RideEventHub. Let's combine these two to create the connection string and run this. Now put this information in a dictionary eventHubReader1. Add the connection string in encrypted form. You can do that using EventHubUtils.encrypt method. Then add the consumerGroup, but just reader1. Let's execute this. This creates an event hub configuration and completes our environment setup.

Building Streaming Pipeline with Delta Lake
Now let's build a streaming pipeline and see what operations it can perform on Delta Lake. Back to Databricks workspace. In the previous clip, we have already added the Event Hub configuration. Now to read the data from a streaming source, let's use spark.readStream. Since we are going to extract from Event Hubs, provide the format value as eventhubs. Next, pass in the configuration settings by using options, and specify the load method. This will create a streaming data frame, rideInputDF. Now let's use display function to show the data in DataFrame. Display function is provided by Databricks. Here, provide the DataFrame, rideInputDF, and let's define processingTime of 5 seconds. This means every five seconds Spark will execute this and pick up new events from the Event Hub. Let's execute this. And it has started running the stream. Notice, while the query is running, there is no new data right now. Now let's switch over to the sample application and run it. Press Enter to start sending the events. Now it is sending events to Event Hub. Back to the workspace, and notice that the events are streaming in. In the dashboard, you can check the statistics like input rows per second, number of rows it can process per second, and so on. For detailed information, you can check the Raw Data tab. Along with the dashboard and raw data, it also shows the actual data. And see, it's getting new events every five seconds. Now we are receiving the data in Event Hub's default format, but where is the taxi ride's data? The body contains the actual rides data but in binary format, and the other properties represent the metadata from Event Hubs. Great, let's stop the query and see how we can transform the body to get the rides data. Now to extract the ride's data, let's first define the schema. Use the StructType method and add the columns you need to extract. Let's execute this. And the rideSchema variable is ready. Now, as you saw, body column contains actual data but in binary format. So first, let's cast the value of body column to string. Next, let's convert the string to JSON using from_json method. So in this method pass the string and the rideSchema that we just defined. And let's define a name for this expression using alias method. So we have casted the binary data to string, converted to JSON, and named it as taxidata. Sounds good? Now from taxidata column, let's select the attributes individually and make them as columns. Again, use the select method. Now to extract data from a JSON type, use column name followed by the attribute name. So use taxidata.RideId to extract RideId value from JSON. Let's run this. Now let's use display function again and execute it. And there you can see that the DataFrame now has actual ride's data. It has RideId and other attributes. Awesome. Let's drop the query in the sample application. Now let's create a Delta table to store this data. Let's name it as TaxisDB.GreenTaxis and define the columns. It has schema similar to YellowTaxis. Specify format as Delta and define the folder path location in Data Lake to store the data. Let's run this. And this creates a nonpartitioned Delta table. Now to start saving the data from stream, use writeStream method on DataFrame, define format as delta, and in table method, define the name of Delta table, which is GreenTaxis. Now notice other attributes, they are specific to streaming. First, there is outputMode. There are three output modes. We'll see where they can be used one by one. Here, let's set it to append. This means only the new rows prepared by Spark will be added to Delta table. Then there is checkpointLocation. Provide the path to Data Lake. This keeps track of which events have been processed. And finally, set the processing time, say, 5 seconds. This means every five seconds Spark will execute this. That's it. Let's run it now. Once again, start the sample application and events are flowing again. And the stream will now start saving data to Delta table. Let's do the count of records and notice the count. Let me run it again, and you can see the count has increased, right? And that's how Delta Lake is continuously appending the data. You can also run select query to check the data. Remember, while the writer is writing the data, here we are parallelly reading the data. Let's stop the query and the application. Now let's create a report to aggregate the rides by vendor ID. So let's create a table, GreenTaxisSummary, and let's have two columns, VendorId and TotalRides. Let's create this table. And the table is now ready. Now, on rideDF, let's use groupBy method and group the rides by VendorId. Then let's count the number of rides using aggregate function and name the column as TotalRides. And let's create a new DataFrame for this. Now let's use the same query on rideSummaryDF to write the data. Now here, the checkpointLocation is different so that this query can separately track its progress. The table name is GreenTaxiSummary, and the outputMode is set to complete. This means with each 5‑second run, the whole table will be overwritten, and the new aggregate values will be added there. Make sense? Let's run this, and also start the sample application. Now this is writing data to summary table. Let's query the summary table. Notice the count of records by vendor. Let's run it again and see the aggregate values are changing. So append mode inserts the new records in the table, while complete mode overwrites the table. And that's how we can write streaming data to Delta table. Make sure to stop the queries and the sample application.

Updating Delta Table with Streaming Data
Now so far, we have appended or overwritten the Delta table with streaming data, but the major benefit is to update the table even with streaming data. Let's see how to do that. Back to Databricks Workspace. First, let's query the GreenTaxis table that we previously created, and you can see pick up events with EventType P are stored separately, and their drop‑related fields are null. On the other side, drop events with EventType D are stored separately, and their pick up fields are null. But we would like the pick up and drop information in the same row for each ride ID. Let's see how to fix this. To do that, let's use truncate command to delete all rows from the table first and run this. Now let's try and update the rows. Currently, Spark's structured streaming does not support direct update to Delta table, so let's see how to do that. Now the code for this is very similar. On the rideDF, use .writeStream method, define the .outputMode as append, a new checkpointLocation for this query, and the processingTime of 5 seconds. Now let's use a sync called .foreachBatch. This sync requires a function to be passed. Let's create a function, writeGreenTaxiEvents, and this function takes a DataFrame as input. Now with each run, every 5 seconds, the .forEachBatch sync will call this function, and all the new records from the source will be passed to this function as a DataFrame called batchDF. Make sense? Now that batchDF will have the records, let's write the code to merge changes into Delta table. First, let's create a temporary view on the DataFrame and call it changes and then write the MERGE statement. You can write it in Python or SQL. You have seen the merge before. Let's merge the records from changes view into GreenTaxis table. Join on RideId and VendorId. If a ride does not exist in the table, then insert it, but if ride does exist, then update the record. Take a pause and see how this has been written. Let's run this, and this creates a function. Now let's run the query. The stream is now running, and start the application again. Now run the application at least for a couple of minutes so that drop events can come in, and there you can see some drop events. Now let's run a query on the table. Notice if the rides have started, drop information is coming as null. But if rides have completed, it's showing both pick up and drop information for the same record. Interesting, right? You can run another query to check only completed rides. Run it a few times and see the number of completed rides will keep growing, and that's how you can update table, even with streaming data.

Transaction Log Checkpointing
Now let's see what is transaction log checkpointing and how it works. Let's say you're writing to our delta table. As you know, for each transaction, an entry is added to the transaction log. Now if you have performed 9 transactions, 9 JSON files have been written from 0.json to 8.json. Another transaction would mean adding 9.json. Now if you query the table, it will read the current version, which is 9, and for that, it will read all the log files till version 9. Now I'm sure you are thinking that if there are a lot of versions, it will read all the log files every time, and this can be a big performance bottleneck, right? Now let's say you write another version 10, but along with writing 10.json, delta writes a checkpoint file, 10.checkpoint.parquet. This file saves the entire state of delta table at this point of time. And when will this happen? A checkpoint file is written every 10 commits. So along with version 20, a checkpoint file will be written 20.checkpoint.parquet. So checkpoints are created to store the intermediate state. Make sense? Now let's say there is another transaction, version 11, and when you try to read the latest data, it will only read the latest checkpoint file at 10 plus version 11, so there is no need to read all the previous logs since their information is stored in the latest checkpoint file. But do you have to do anything for this? Nothing. This is handled by Delta Lake. Also notice that Delta Lake has default log retention duration of 30 days. When checkpoint is written, any log files older than 30 days are also removed. Back to Databricks workspace. Since we have ran streaming job on GreenTaxis table, let's see the history of this table, and you can see there are a lot of versions that have been created. Now let's go to Azure portal and open the folder location of green taxis. Let's open _delta_log folder. Notice that there is a file called 10.checkpoint.parquet. This contains intermediate state of table till 10 transactions, and then there is 20.checkpoint.parquet. This has the state of table from 0 to 20 transactions. And there is also 30.checkpoint.parquet. So Delta Lake automatically optimizes the read performance by creating these checkpoint files.

Handling Concurrency
Handling concurrency on Delta Lakes is extremely challenging. Let's see how Delta Lake handles concurrency between multiple writers. As discussed earlier, Delta Lake uses optimistic concurrency. Let's see how that works. Let's say there is a Delta table. The table contains two columns, Id and Name. One file has been written with one record, and the log file has been added. Now let's see how simultaneous inserts work. There is a writer called writer A, which wants to insert one more record with Id=2. And there is a writer B that apparently wants to insert a record with Id 3. Now both the writers record the current version, which is 000.json. And now, both will write their partition files. Writer A writes a file with Id 2, and writer B writes a file with Id 3. Once Spark files are written, now both will try to commit the changes to transaction log. In this case, both will try and write to 001.json file. Let's say writer A is able to commit the log first. So because of this, writer B is not able to commit the log file. This is how optimistic concurrency works, and that's how Delta Lake provides isolation to transactions. Make sense? But Delta Lake is smart. In this case, it will optimistically retry to add a log entry, so it adds 002.json as the new log entry. Remember, this is only possible in case an insert fails, and that's how multiple inserts can work concurrently. Now let's see another scenario. Once again, there is a Delta table with Part File 1. And there is a log file, which is Add Operation for Part 1. Now let's see how simultaneous updates work. On one side, there is writer A, which wants to update the record. And on the other side, there is writer B that wants to update the same record. So once again, both record the current version, 000.json and will now right their partition files. Writer 1 writes a part file with updated record. Their name has changed from A to A1. Writer 2 also writes a file, and it's trying to change the Name to B1. Once done, both will try to commit the changes. Assume that writer A commits first. It mentions that Part 1 is no longer required, and Part 2 is a new file. But this time, writer B won't be able to commit the log since Part File 1 is now considered as removed. So that's how concurrent updates are handled by Delta Lake. Remember, no retries are performed in case an update command fails. So let's see how configs are handled during parallel writes. If there are two writers and both want to insert the data, there is no conflict, and they are silently handled as you saw in the example. Then, if one writer is inserting while other is performing update, delete or merge operations, this is also silently handled, and there is no conflict. And finally, if both writers are trying to update, delete or merge the data, this can cause conflicts if both are working on the same set of files, and one of the writers will fail and throw an error, but you can resolve it easily by making sure that both the writers are working on separate partitions. Sounds good.

Demo: Handling Concurrency
Now let's see how to handle the concurrency. Back to Databricks workspace, I have created a new notebook. Let's set up the environment first. We have done many of these steps earlier in the module. First, let's define the connection string for Event Hub namespace. Use the connection string you copied earlier in the module. Second, let's define the schema for taxi rides, and third, let's recreate the table. Drop the files from datalake and then create the table. Notice, it's a non‑partition table. Finally, let's run all the above cells by selecting the option, Run All Above. Now, let's set up two processes that will write at the same time to Delta table. We'll perform four steps here, as we did earlier. Prepare the Event Hub configuration, read the streaming data from Event Hub, and then extract the actual ride's data from Event Hub. Now the first process will only work with VendorId = 1. So let's define the Event Hub configuration and read the stream from Event Hub, then extract the actual data from Event Hub schema. You have seen this before. And finally, let's apply a filter that this process will only work for VendorId = 1. Let's run this, and this creates a DataFrame, rideDF1. Let's do the same for process 2. This will process data for VendorIds other than 1. Notice, we are applying a filter, VendorId <> 1 and creating a DataFrame, riideDF2. Let's run this. So now we have two writers, which are working with different datasets. Make sense? So let's see the first scenario where both the processes will try to insert the data into the table. Let's run the first process and run the second process. I'm sure you have noticed both are using append mode. This will insert the data into Delta table. Back to the command line, let's start the sample application, and the events are now flowing in. Both of them are now inserting the data to Delta table. Let's go to the table now. Notice, the data is being added for VendorId 1 by process 1 and for other VendorIds by process 2. Remember, this is a non‑partition table. So files are getting added at the same location, but still, two inserts and one read operation here are not interfering with each other. Right? Let's stop these queries in the sample application. Now let's see how simultaneous updates work. Both processes will try to update the same table, which is nonpartitioned, at the same time. To test it, let's truncate the table first. As you saw earlier in the module, let's create a function to handle updates and merge the data. This function is for process 1. Query is exactly the same. But since process 1 is only working for VendorId 1, we have added Vendor ID = 1 filter in the MERGE statement. Let's run this. Let's create one more function for process 2. And in the MERGE statement, notice the filter where VendorId <> 1. This means both processes will work with different sets of data and even using different MERGE statements. Let's run this, too. Now let's run the first process. Notice it is using .foreachBatch sync, and we'll call function 1 every 5 seconds. And function 1 will take new records and merge it into Delta table. Make sense? In the same way, let's run the second process. This is using function 2, which will be invoked every 5 seconds, and the function will perform the merge. Now let's start the sample application, and events are now flowing. Back to workspace, notice within a few seconds, one process has failed. Let's see the error. It's a ConcurrentAppendException. Files are added to the root of the table by a concurrent update. This means there is a conflict while updating the data. Why? Because they are updating data in the same partition. So two updates, deletes or merge operations can easily cause conflict. For now, stop the other query and the application. So how to solve this problem. This can be done by making sure that writers work with different partitions. Let's see that. First, let's remove the Delta table files from datalake, and let's recreate the table. All the things remain the same. But this time, let's partition the table by VendorId. Let's execute this. Once again, let's run the first process and then the second process, and finally, let's start the application again. Events are now flowing in, and this time, you can see the processes are running well. No failure so far, and you know why. Both processes are updating different partitions, so there are no concurrency issues. Let's run a query to find rides, which have completed. To do that, let's filter records where pick up time and drop time both are not null. Let's run this. And you can see these trips have completed. When the rides started, only pick up information was added, and when it finished, drop information was updated. So that's how concurrency is handled on Delta Lake.

Summary
In this module, you first saw how streaming is supported on Delta Lake. In fact, it is integrated with Spark Structured Streaming, so you can store batch and streaming data together. And you can use Delta Lake as a streaming source and a streaming sink. Then you saw how to set up the streaming environment. We used a sample application to generate events, and pushed them to a stream ingestion service, Azure Event Hubs. Now to read data from Event Hubs, you can configure it in Databricks using libraries. Next, you saw how to build an end‑to‑end streaming pipeline. While writing the streaming data, append mode adds only the new records in the table, and complete mode overwrites the whole table. But Delta Table can also be updated with streaming data. To do that, you can use foreachBatch sink along with the merge statement. Then you saw transaction log checkpointing. Delta Lake automatically stores the intermediate state of log in a checkpoint file. This increases the read performance since it reads the latest checkpoint file instead of reading all the prior log files. Finally, you saw various concurrency scenarios with multiple writers performing inserts, updates, etc. And partitioning can help to improve concurrency. Now in the next module, let's look at how to optimize queries with Delta Engine in Databricks.

Optimizing with Delta Engine in Databricks
Module Overview
Hey everyone! Welcome to this module on Optimizing with Delta Engine in Databricks. So here, we'll first understand what is Delta Engine and what are its components. We'll then look at various optimizations, like how query performance can be improved with stats and data skipping, how to optimize Delta table, and use a technique called Z‑ordering, and then see how to clean up unused files using Vacuum. Now, Delta Engine also provides features for auto optimization that we'll discuss here. And next, we'll discuss what is Photon Execution Engine and how to work with it. And finally, we'll see what is Delta Cache and how to use it.

Understanding Delta Engine
Let's start by understanding what is Delta Engine? Delta Engine is only available in Databricks. It's a high‑performance query engine, which is built into Databricks Runtime. It can help to process, as well as query the data faster than the Spark engine, so it's a great engine for SQL workloads. Now using this, you can query any data format, including Delta format. Now there are three components of Delta Engine. First, it has a built‑in query optimizer. Second, it has a new and extremely fast execution engine called Photon. And third, it has a caching layer called Delta cache. You can use these components together or even independent of each other. Let's discuss them one by one. First, there is query optimizer, it has optimizations like data skipping, bin packing and Z‑Order optimizations, vacuum of Delta tables. While this is not exactly a part of Delta Engine, but it's an extremely important optimization. Then there is auto optimization, data transformation optimizations, Join and Low Shuffle Merge optimizations, and a few more. We'll discuss some of them in this module. Notice, while these optimizations belong to Delta Engine, and you know Delta Engine is only available in Databricks, but now, some of these optimizations are making their way to open‑source Delta Lake project. Now let's look at Delta Engine features one by one.

Improving Performance with Statistics and Data Skipping
Now let's see the first optimization, that how performance can be improved with statistics and data skipping. As you know, whenever we write to Delta table, an entry is made to transaction log. But with each log entry, certain statistics are computed and stored along with it. Let's see that. Here is a sample transaction entry, and I have only kept limited information here. You have seen commit information before. It contains operation metrics. And notice the number of files committed is 2 and the output rows are 200. Now for each file that is added, stats are computed. Here you can see the number of records in first file is 101, and along with that, it is storing the minimum values for each column, as well as maximum values for each column. Interesting, right? In the same way, the second file stacks are also stored, and this contains number of records, minimum values, and maximum values. Now that you have seen statistics, let's see how data skipping works. Let's say there are three part files created for Delta table, and there are two columns, EmployeeID and Salary. Now, for the first file stats are computed, EmployeeId has minimum value of 1 and maximum of 3. And minimum salary is 10,000, while max is 15,000. For second file, EmployeeId is between 4 and 5. and salary is between 18 to 21,000. Finally, file 3 has salary between 14 to 19,000. All these stats are stored in the transaction log. Now let's write a query, SELECT * FROM employees WHERE salary BETWEEN 16000 and 17000. Let's see how Delta lake will process that. It first checks the stats for file one. Since max salary present in the file is 15,000, so it does not fall in the range of the query, so this file is ignored. Then for second file, the minimum value for salary is 18,000. And notice, this is also outside the range of the query, so this file is ignored as well. Finally, for file 3, the salary range is 14‑19,000, and this falls in the query range, right? So file 3 will be considered for extracting the data. Now let's see the file. There is only one record that is between 16 and 17,000, so this will be the output. But carefully notice, we have skipped two files completely, and only reading data from one file. Make sense? So this means, for data skipping, Delta Lake checks the stats in the log, then it reads only those files which fall within the range of filter conditions. And rest of the files are skipped or ignored. And because of this, query performance can significantly improve. Let's see that in action. Back to Databricks workspace, I have created a new notebook. First, let's clean up folders for YellowTaxis that we previously created. Let's do that using dbutils remove command. Next, let's drop the tables. Let's run this as well. Then, let's define the schema for YellowTaxis. You have seen this before. Now let's read the YellowTaxis file by applying the schema and setting header as true. Then, let's create three additional columns in the DataFrame, which is TripYear, TripMonth, and TripDay. Let's run this, and this creates the DataFrame yellowTaxisDF with three additional columns. Now let's save the output to data lake in Parquet format, and this time let's partition the data by PickupLocationId. Notice the format is parquet and the table name is YellowTaxisParquet. In the same way, let's save the data in Delta format. This query is exactly the same as previous one, except the format is Delta. And let's call this table as YellowTaxis. Let's run this. This will take a few minutes to write the output since there are a lot of partitions, Let's check the table history. Remember, this command only works for Delta tables. Let's run this. Notice more than 2,500 files have been written containing close to 10 million records. Now let's see how both tables perform while counting the number of records. Let's run the query on Parquet table. Notice the time it took. And now let's run it on Delta table and see there is a huge performance difference. Why? This is because Parquet is opening and scanning each file to count the records. On the other hand, Delta is simply keeping count of records from stats. This means with growing data, time to query Parquet table will increase, while it will remain almost the same for Delta table. Sounds good? Now let's compare the performance when querying a single record. Let's select from Parquet table, WHERE RideId is 67. And run this. Once again, it took a lot of time. Again, it scanned all the files to get this record. Also notice, Databricks is giving a hint. This query contains a highly selective filter. To improve the performance, convert the table to Delta. Why? Because then data skipping can be used. Let's see that. Let's run the same query on Delta table YellowTaxis. Let's run this, and there you can see it took very less time. Almost all the time was actually spent in scanning the stats since RideId is unique and will only be found in one file, so only one file was opened instead of 2,500 plus files. And that's how stats and data skipping helps to significantly improve quality performance. We'll see more complex queries in the next clip.

Optimizing & Z-ordering Delta Table
Now let's see the feature of optimizing and Z‑Ordering Delta table. Now there are two optimized options. First is bin packing. This is also called as file compaction optimization. And second, there is Z‑Ordering optimization. Let's look at them one by one. Let's say you are storing customer data in Delta format, and there are 200 partition files. And these are small‑sized files. Why? This is because if you're continuously performing transactions on a table like INSERT, UPDATE, and DELETE, this adds multiple small files over a period of time. But reading these files involve a lot of overhead like opening the file, checking the schema, etc. and this closed down the performance. This is what is known as small files problem. Now in Delta Lake, this can be stored using bin packing or file compaction feature. To use it, you can run OPTIMIZE command on the Delta table, and this will take data from small part files and pack it into bigger size files. So the data is compressed and now stored into big‑sized, new part files, and they are evenly balanced. Now the default max size as set by Databricks is 1 GB, but this can be changed to a setting. Remember, the new files P201 and 202 are created. The previous part files are not deleted so as to support time travel. As a quick tip, if you're performing a lot of operations on your table, run the OPTIMIZE command periodically to compress the files. Make sense? Now let's see the second optimization. Let's say there are three part files in the Delta table for customers. The table has two columns, Id and City. Now let's run a query, SELECT * FROM Customer WHERE City = 'Delhi'. Notice that the records for Delhi is present in all three files. This means all three files should be opened to get the data, right? Now these scenarios can be optimized using Z‑Order optimization. To use it, you can again run OPTIMIZE command, but this time with the clause ZORDER BY (City). Since you are running OPTIMIZE command, it will create new files. But this time, data will be sorted by City column and then stored in the files, and that makes the data colocated based on city. Now let's run the same query on the table, and you can see data is being picked up from one file only. And the other file is being ignored because of data skipping. So this means when you run OPTIMIZE ZORDER command, it performs bin packing. But along with that, data is sorted and colocated in the files based on ZORDER columns. This is very similar to clustered index in the database. This technique improves performance because more files can be skipped now. As a recommendation, use those columns in ZORDER, which are frequently used in filters, joins, and grouping queries. Remember, bin packing and Z‑Ordering both our CPU‑intensive, costly operations. It's because data is physically reorganized into a new set of files. So run these commands during quiet business hours. Back to Databricks workspace. Let's see how these commands work. In the previous clip, we created Parquet and Delta tables for yellow taxis. These tables are partitioned by PickupLocationId. First, let's run an aggregate query on Parquet table. Here, let's apply filters for TripYear, Month, and Day. This means that data will be queried across partitions. Let's run this, and notice the time for this. Now let's use the same query on Delta table and run this. You can see it took significantly less time, and you know it. This is all because of data skipping. There are 2,500 plus files for this table, but we are only querying some of them. Now let's open data for the location in Data Lake. You can see one folder for each partition. Let's open the folder for PickupLocationId=100. Notice there are 10 small files. If data related to a query is present in all these files, then Spark will have to open all these files. This can cause performance degradation. So to compress these files, let's run OPTIMIZE command. Just use OPTIMIZE TaxisDB.YellowTaxis. This will compress all files to optimal size across each partition, but optionally, you can provide conditions. Think about it. If data has been modified in only one partition, why to optimize all partitions, right? So let's optimize data for partition, PickupLocationId = 200. Let's run this. Notice one partition has been optimized, 10 files have been marked as removed, and one new file has been added. Now switch over to Azure portal, and let's check the partition. You can see all previous 10 files are there. This is to support time travel. And a new file has been added with more than 5 MiB data. This one single file now contains all the records. In the same way, you can optimize the whole table without putting a WHERE clause. And here is another option, ZORDER. Let's perform bin packing and also physically sort and store the data by TripYear, TripMonth, and TripDay. Let's run this. This could take a few minutes depending on the volume of data. Notice, all 256 partitions have been optimized. More than 2,500 files have been marked as removed, and 255 new files have been added. Sounds good. So let's use the same GROUP BY query again, this time on optimized data. Let's run this, and you can see this performed much better than previous queries. In fact, Z‑Ordering will be even more effective with higher volumes of data. So that's how Delta table can be optimized with bin packing and Z‑Ordering.

Cleaning Files with Vacuum
Now let's see how to clean up files from storage that are no longer required. Let's take an example. Say you write two part files to a Delta table, then 000.json is added to the log. The log file stores that Part 1 and Part 2 were added. Now let's delete the record where Id is 1. The Id 1 is present in Part 1. Now Delta will create a new Part file 3. It will copy the unchanged row from Part 1 to 3. This means a row with Id 2 is copied. Then, 1.json is added to the log, and it stores that Part 1 is removed and Part 3 is added. You have seen this before. And what about Part 1 now? Yes, it can be used for time travel. But let's say you don't want to use that version anymore, and want to clean up the old files. This is where you can run the VACUUM command. Since according to the latest transaction, Part 1 is no longer required, the VACUUM command will delete Part file 1. So the VACUUM command physically removes those files from Data Lake, which are no longer referenced in the latest transaction log, and any files that are older than a retention threshold. The default retention period is 7 days. In other words, only those files are kept that are referenced in the current version, or if they are related to transactions in the last 7 days, and the rest of them will be deleted. Makes sense? Remember because part files are removed, this affects the time travel. As a quick tip, first decide how long back you want to perform time travel. Based on that, set the retention period and then perform the vacuum periodically. Let's see how this works. Back to Databricks workspace, let's continue to use the same YellowTaxis table. First, let's check the table history, and there you can see CREATE and OPTIMIZE operations. Let's switch over to the Azure portal and open the YellowTaxis folder. Let's take the folder PickupLocationId=100. These 10 files were used before running the OPTIMIZE command. And this one compressed file was created after the OPTIMIZE command. So the first 10 files are not used in the latest version. Now let's run a VACUUM command to clean up the files. Run VACUUM on YellowTaxis, then specify how many older versions you want to retain. Specifying 0 hours will delete all the previous versions. Remember, if you don't specify retention hours, by default, it's 7 days, and then use DRY RUN clause. This clause will not delete the files, but only return the list of files to be deleted. Let's run this. And it's throwing an error, Are you sure you would like to vacuum files with such a low retention period? So Databricks is blocking us from deleting all prior versions. So it's important to use the right retention period. But if you still want to delete all previous versions, you can set retentionDurationCheck condition to false and run this. Databricks will no longer warn us. Now let's run the command again, VACUUM DRY RUN, and these are the files that will be deleted by vacuum, as they are not referenced in the latest version of the table. So finally, let's remove the DRY RUN clause and run the VACUUM command. It may take a few minutes to complete because it's physically deleting the files. Let's switch over to the Azure portal, and let's refresh. Now you can see only one file referenced in the latest version. Previous files have been physically removed. Sounds good? Now let's take the history again. And there you can see two new versions, one for VACUUM START and the other for END. But as we discussed, this will impact time travel because old files are no longer available now. So let's query the VERSION 0 OF table and run this. There you can see the exception because physical partition files referred by VERSION 0 are no longer available. So that's how vacuum can be used to clean up files, but remember to define the right retention threshold and the impact of vacuum on time travel.

Enabling Auto Optimization on Delta Table
Delta Engine also provides auto‑optimization features for Delta tables. There are two Auto Optimize features. First, there is Optimized Writes. This automatically performs bin‑packing while writing the data. And second, there is Auto Compaction. This also performs bin‑packing, but after writing the data. Let's understand them in detail. Let's see how files are typically written to Delta table. Let's say you have a Spark cluster and a Data Lake. Now you want to run a job to process sales data and write the output to the Sales folder in Data Lake. To do that, Spark will create multiple tasks to process the data in parallel, and these tasks will parallelly write the partition files in the Data Lake folder, but these can be smaller files. That's where you may use the OPTIMIZE command to compact them. Now let's see how it works with the feature of Optimized Writes. Once again, say you want to run a job to process sales data, so Spark will create multiple tasks. Now, if Optimized Writes is enabled, Databricks will try to dynamically optimize partition sizes and may create few additional tasks to do that. Once these tasks are processed, files of optimal size will be written out. So with slight additional overhead, now you have optimally sized files, and read queries can work better. Interesting, right? So in case of Optimized Writes, Databricks dynamically tries to create partition files of around 128 MB. So with slight overhead in writing, you can prevent the creation of small files. And then there is no need to run the OPTIMIZE command later. Remember, the OPTIMIZE command tries to create 1‑GB files, but since this operation happens while writing, so optimal size has been set to 128 MB. And if you still want to optimize to a 1‑GB size, you can run OPTIMIZE command. Now think about this. If there is streaming data, it can create a lot of small files. This feature can help to avoid that. Then, if you're performing DML operations, you may use this setting. Then the second feature is Auto Compaction. This performs bin‑packing after writing. So immediately after writing data to a table, Databricks further checks the partition files. And if there are any non‑compacted files in the Delta folder, they will be compacted to around 128 MB. But this feature only runs if there are more than 50 small files. Once again, you can avoid running the OPTIMIZE command later. Now this can also be used when you are loading streaming data to a table. Also, if you plan to run the OPTIMIZE command after each write, you can use this feature instead. Let's see how this works. Back to the Databricks workspace. Let's create a table, YellowTaxis_NonOptimized. It is partitioned by PickupLocationId. Let's run this. Now let's add data to this table. For that, let's define the schema for yellow taxis. Then let's read a file from Data Lake using the schema. Let's run this. And this creates a DataFrame. Now let's add data from this DataFrame to the table. Let's run this. Now that the data is saved, let's switch over to Azure portal. There you can see multiple partitions based on PickupLocation. Let's open the partition 100, and as expected, there are multiple small files. It will be similar for all the partitions. Now let's create another table, YellowTaxis_Optimized. It has the same schema. Now just notice how you can define table properties. And there you can specify delta.autoOptimize.optimizeWrite is true and autoCompact is true. This will enable auto optimization at the table level. And now, let's add data to this table from the same DataFrame. Let's run this. Again, let's switch over to Data Lake. This table also has partitions. Let's open partition 100, and notice it just has one bigger file, so it used the feature of Optimized Write, and while writing, the smaller files were compacted. Very useful. Remember, Auto Compact will come into effect if there are more than 50 small files. In the last module, we created GreenTaxis table to store streaming data. Now recreate that table by adding these optimizations and table properties, add streaming data to it, and see the effect.

Working with Photon Execution Engine
Now let's see the second component, the Photon execution engine, and how to work with it. Photon is a native vectorized query engine built by Databricks and only available in Databricks. It's a new engine written in C++, which takes the benefit of modern hardware for accelerating query performance. Notice it can work with Delta, as well as Parquet‑based tables, but with Delta, it works even better. Then, it is much faster than Spark, but do you need to change your code? No, because it's fully compatible with Spark APIs, so you can run the same Spark code on Photon engine. Now to use it, you can select Databricks Runtime enabled with Photon. But remember, it is costlier than Spark‑based clusters. So let's see how you can set up a cluster with Photon. Back to Databricks workspace. To create a new Photon‑based cluster, go to the Compute tab, and click on Create Cluster. Let's provide a name, PhotonCluster. You can use any Cluster mode. Let's select Single Node. Now select a Databricks Runtime version. There, select the option of Photon, and there you can see runtimes, which support Photon, so let's select one of them. Currently, Photon uses bigger sized machines, so let's select one of them and set the termination time to 30 minutes. Let's click on Create, and this will set up a Photon‑based cluster. Simple, right? Now to compare the performance, let's create a regular Spark cluster with the same configuration. Click on Create Cluster, provide the name as SparkCluster, select mode a Single Node, select the same Databricks Runtime version, but without enabling Photon. Select the same node type and create the cluster. Once both the clusters are ready, let's go to the notebook, and let's run queries on YellowTaxis table. To run queries using Spark Engine, make sure you are connected to SparkCluster. Now let's use a SELECT query with aggregations and filters. Let's run this. Run it one more time to make sure there is no boot up time added. Notice the time it took. Now let's attach the notebook to the Photon cluster. Let's run the same query, and one more time. There you can see the Photon engine performed much better, right? With bigger tables, it can perform even better, and this can help teams to quickly query and analyze the data.

Using Delta Cache
Now let's look at the third component, the Delta cache. Now there are two types of caching available in Databricks. First, there is Apache Spark cache. This is a feature of Spark. And second, there is Delta cache. This is a part of Delta Engine. Here, we'll be focusing on Delta cache Let's see how Delta cache works. It supports both Parquet and Delta formats. Now when you want to read any data for the first time, the files are read from storage and then the copy of those files are stored on the local disks of the cluster machines. This is called Delta cache. Then, if you try to read any data files for which are presenting Delta cache, it will be served from the cache. And this can significantly improve query performance. Now, how is it evicted? Delta cache automatically detects files that have been modified or deleted after they are cached, and it evicts them from cache. Or if you restart the cluster, all cached files are removed. So let's see how you can use Delta cache. Now while creating the cluster, you can select Delta cache accelerated VMs as worker machines. There, the caching is enabled by default. Or if you are using any other types of machines, you will need to explicitly enable the cache. Let's see how to use it. Back to the workspace. Now to enable Delta cache, you can create cluster with Delta accelerated VMs. To do that, click on Create Cluster. Let's check the worker types, and notice there are certain Delta cache accelerated VMs. If you use them to create a cluster, Delta cache will not just be enabled, but it'll be compressed and optimized as well. Now, by default, cache can consume 50% of space on SSDs, but you can change that setting. Now I'm not going to create this cluster. In fact, let's see the other option. Now, let's use a cluster with non‑Delta cache VMs. Let's check the setting if Delta cache is enabled. To do that, use spark.conf.get and pass the setting spark.databricks.io.cache.enabled. Let's run this. And you can see, Delta cache is disabled by default. Let's check the performance and run an aggregation query. Run it one more time and notice the time it took. Now let's enable the cache by using spark.conf.set, add the Delta cache setting, and set it to true. Let's run this. Now that the cache is enabled, let's run the query again. Since this is the first execution after enabling cache, files will be cached on the disk. So notice the time for this. And now, let's run the query for the second time. You can see the time for this has highly reduced because it's not picking up the files from storage, but using cache instead. So without much effort, you can enable Delta cache and get great query performance. In fact, if you want great performance, even with the first‑time queries, you can manually cache a subset of data. It's very easy to do that. Use CACHE keyword in front of SELECT query. Now this query will put all those files in Delta cache, which has data of TripDay = 1. Let's run this. So after this, if you run any query which is going to use the same files present in Delta cache, performance will automatically improve. So this is an extremely useful feature of Delta Engine.

Summary
In this module, you saw that Databricks has a high performance query engine called Delta Engine, and it is built in into Databricks Runtime. It has three components, a query optimizer, Photon execution engine, and Delta cache. The query optimizer has got various optimizations. Now you can improve the quality performance using stats and data skipping. As you saw, with each log entry, stats are computed and stored. This includes count of records, and min and max were used for each column. So if you run a query, only those files are open that contain data, and the rest of them are skipped. Then, you can also optimize and Z‑Order Delta table. Optimize command takes data from small files and creates evenly distributed new files. This is called bin packing or file compaction, and you can use Z‑Order to physically sort and then store the data in files. And since data becomes colocated, queries can take advantage of that. Then, if there are any unused files, it can be cleaned using vacuum command. You can specify the retention duration for files, but by default, it is seven days. Next, you saw auto optimization in Delta Engine. It has a feature, optimized writes where bin packing is performed while writing the data. And another one called auto compaction where bin packing is done after writing. Now the second component is Photon execution engine. This is a native vectorized query engine within C++. It's fully compatible with Spark, but works much faster. And finally, you saw Delta cache. This caches data on local disks and helps to improve query performance. So you can highly optimize your workloads with Delta Engine. Now, in the next module, let's look at how to build a lakehouse architecture.

Building a Lakehouse Architecture
Module Overview
Hey, everyone. Welcome to this module on Building a Lakehouse Architecture. So first, we'll discuss what is lakehouse architecture? Why is it required, its objectives, and components. Then to handle the storage for lakehouse, there is Delta architecture, so we'll see what are its components. Next, we'll see another feature of Delta Lake, the change data feed, and how to use it. And finally, we'll implement the Delta architecture for batch data using change data feed. So let's get started.

Understanding Lakehouse Architecture
Let's first understand what is lakehouse architecture? Lakehouse intends to bring the best of data warehouses and data lakes together. Great! But let's stop here and take a step back. Two to three decades back, most organizations were only working with structured data sources. So if a business analyst wanted to do self‑service BI or needed reports for analysis, we would get the data into a staging environment, clean and transform the data, and then load it into a data warehouse. It can then be used for self‑service BI and reporting, which would make the analyst happy. So data warehouses contained highly curated and structured data. You get a very reliable data model built with reporting requirements in mind. You also apply data governance principles, so data remains of high quality throughout its lifecycle. And of course, over a period of time, the tools and the ecosystem have highly matured as well. But here is the problem. Structured data sources now only accounts for 20% of an organization's data. There is data available in a variety of file formats like CSV, JSON and log files. There is data from NoSQL databases, from audio and video files, and then there is streaming data as well. So this unstructured and streaming data sources accounts for 80% of an organization's data. So if a data scientist wanted to do machine learning or data science, it could not be done through data warehouses. This is where came data lakes. So we would push the data into a data lake from where it can be utilized for machine learning and data science, and that would make the data scientist happy. So as you have seen, data lakes has many benefits. You can store the data in open file formats instead of proprietary ones. The raw historical data can be stored in native format, and this is because it's inexpensive and can scale to store vast amounts of data. You can use it to do ML and data science. But as we discussed earlier, there are challenges with data lakes. Unlike data warehouses, it does not provide ACID guarantees. If not governed properly, data lakes can quickly turn into data swarms causing reliability issues, and because of this, it's not suitable for BI workloads, and that's why organizations are using both. You can bring all types of data in Data Lake. Data can then be enriched by combining data from various sources. Once done, a part of data can then be stored in Data Warehouse. It is further curated, according to business requirements, which can then be used for BI and reporting purposes. Sounds great, right? Now this architecture is also known as modern data warehouse architecture, but there is an overhead involved here. If you carefully notice, now you have to manage two systems independently, Data Lake and Data Warehouse. This also means you are now maintaining multiple copies of the data. And this additional ETL activity between lake and warehouse not just increases time, but can have reliability issues as well. That's where it was thought to bring both these systems together, and this is what is referred to as a lakehouse. So we can get all the data into a lakehouse, enrich and curate it at the same place, and then make it available for a variety of use cases. Interesting, right? So, as we discussed earlier, a lakehouse intends to bring the best of data warehouses and data lakes together. This includes adding data warehouse‑like features like reliable storage, governance, and management on top of low‑cost Data Lake. Now let's look at the objectives of a lakehouse. Now bear with me for the next couple of minutes, as it's a long list. So first, it should be able to handle all types of data, whether it is structured, semi‑structured or unstructured data. It should maintain data integrity and consistency. This means it should be able to support ACID guarantees. It should reduce the ETL activity as much as possible, and you should be able to directly correlate data from different data sources. And since it's one single system, there should not be multiple copies of the data. Make sense? Then, metadata management should be centralized for easy searching of data sources, tracking the datasets, and sharing them. So we are halfway through the list. Let's look at the other ones. Now the data should be stored in open file formats like Parquet or CSV instead of using any proprietary format. And here is an interesting aspect, storage and compute should be decoupled. You should be able to store as much data as you want and pay for it separately. On top of it, you should be able to use any compute, and it should scale independently of storage based on processing requirements. The lakehouse should have integrated security and governance controls so that data can be classified and protected, and we talked about it previously. It should be able to handle multiple use cases, whether it is BI, machine learning, data science or streaming all in one place. And finally, of course, it should be cost‑effective. So you can see, it's a long list of objectives that lakehouse is trying to achieve. Now let's see the layers of a lakehouse. First, there is storage layer. In a lakehouse, you can store the data in a data lake. For that, you can use any cloud storage like Amazon S3, Azure Storage or Data Lake, Google Storage, etc. The cloud storage options are cheaper and highly scalable. Here, you can store all types of data, whether it is structured, semi‑structured or unstructured data, and it can be batch, as well as streaming data. Next, there is transactional layer. This layer should handle metadata and maintain data integrity and consistency, and this is where you can use Delta Lake. As you saw, Data Lake storage provides various features like it stores metadata, provides ACID guarantees. It can maintain data versions and allows to perform time travel. You can perform DML operations, enforce schema, define constraints, and do much more. You have already seen that. So Delta Lake provides features similar to a data warehouse, on top of Data Lake. The next layer in lakehouse is a query engine. Now you can query Delta Lake using a variety of compute engines like Apache Spark, Azure Synapse Analytics, Amazon Redshift, Presto, and Snowflake. In fact, Delta Lake is finding traction with many compute engines. But of course, you can also use Spark on Databricks, and then there is Delta Engine on Databricks. It provides Photon a vectorized query engine, Delta caching to speed up the performance, and a lot of built‑in optimizations. So you can see, all components we have discussed in the course are coming together in lakehouse. And finally, it should provide data governance and handle security. Depending on tech stack, there are many products available for this, for example, Azure has a service called Purview to provide governance. In the same way, Databricks has a new product called Databricks Unity Catalog for data governance. If you want to share Delta tables, it provides Delta Sharing protocol. Unity Catalog also maintains data lineage for better tracking. Then, if you want to build automated ETL pipelines, you can use Delta Live Tables. Finally, Databricks is fully integrated with Azure Active Directory for robust security. You can even set up fine‑grained access controls on Databricks components like clusters, jobs, etc, and then you can control access to Delta tables and even set up row‑level security. And then multiple use cases can be served from it. So a lakehouse can act as a one single platform to serve your data needs.

Inside Lakehouse: Understanding Delta Architecture
Let's dive further into Lakehouse and understand what is Delta architecture. Now, previously we discussed how Lakehouse can be used to store all types of data. And this data can be served for multiple use cases. But how is this data stored and how can we ensure data quality? This is where comes Delta architecture. Delta architecture is a part of Lakehouse that focuses on the storage part. It intends to store the data in a way that can help improve quality of data and so different personas like data engineers, data scientists, business users, etc. This is also called as medallion architecture. Now you might need to extract data from various structured, unstructured, or streaming data sources, and solve this data for various use cases. Let's see the Delta architecture. Here, you can create a Bronze layer in data lake. This is where you can ingest and keep all the raw data as it is. This includes even the streaming data. This data can be used by data engineers for processing and reprocessing if required. So this becomes the single source of truth for the Lakehouse, even if data in the source has been removed. Now remember, you can keep this data in Delta format or even native formats. Now from the Bronze layer, data can then move into Silver layer. You can clean up, filter, and transform the data from Bronze layer and then store it in granular format in the Silver layer. Think of this like an enterprise data warehouse layer. This definitely can be in Delta format. You may also combine data from various tables and update the data with additional information. This kind of granular, cleaned up data can be extremely useful for ML and data science scenarios. And then, the data can move into Gold layer. This is where you can store aggregated, business‑specific data in Delta format. Think of this like data marts from the data warehousing world. In fact, from the same data, you can create multiple tables or views to cater to different business requirements. So this becomes an optimized query layer, which can work for most business users. You can even implement fine grained security on the data here. And that's why this can be extremely useful for BI and reporting purposes. So that's how Delta architecture can help to improve data quality and serve different use cases. But the question is are all these layers, like Bronze, Silver, and Gold, mandatory in Delta architecture? No, remember this is just for reference architecture. Some projects even set up Platinum layers. So it completely depends on requirements, which layers you want to implement. You will shortly see how to implement this.

Using Change Data Feed
Before we implement Delta architecture, let's understand how change data feed works in Delta Lake. Change data feed provides row‑level changes being done on the table. Let's say there is a Delta table with two columns, Id and Name, and it has two records. Now let's insert a row with Id equal to 3, and this adds the row to the table. For this operation, Delta table generates a change data feed. The feed contains Id and Name columns, but along with that it has Change Type, which is set to Insert; time of operation, let's say it's 10:00 A.M.; and the version number of the table, let's say it is 2. Now let's run a delete operation and delete the row with Id = 2. This removes the record from the table. And notice the change feed. It contains all the column values of the row that has been deleted. Change Type is Delete, and then there is time and version number. Now let's run a query to update the value from A to AA where Id is equal to 1. This updates the table. Now, in the change feed it provides two records, one before update and one after update. The record before update has older values and its change type is Preimage. The record after update has newer values and Change Type is Postimage, and both records will have the same time and version number. Sounds good? To use change data feed, you can enable it at the table level. Now for each transaction, changes are recorded, like the rows that have changed; and its metadata, like operation, whether it's insert, update, or delete, and the timestamp. Now these changes are stored in a subdirectory in Delta Lake folder called _change_data. Remember, only update, delete, and merge are logged into this directory, and not the insert statements. And you can read these changes either through batch or streaming queries, so you can use this if you want to incrementally merge the changes from one table to another like in Delta architecture or you can use it for auditing purposes. Let's see a quick example. Back to Databricks workspace, let's use a new notebook and create a simple table, PaymentTypes. It has three columns, Id, Name, and IsEnabled. Now notice, to enable change data feed on a table, the only thing you have to do is set a table property, delta.enableChangeDataFeed, to true. Let's run this, and change data feed is now enabled. On the table, let's run an INSERT statement, and run another one. Now I'll check the table history. There you can see three versions, one for creating table, and next two corresponding to two INSERT queries. Now to get change data feed, you can use table_changes method in SQL. Let's use SELECT * FROM table_changes, pass the name of the table, and the version number from where you want to see the changes. This means you need to store version numbers somewhere if you want to incrementally treat the data. Let's run this. You can see three column values, the _change_type as insert; the corresponding _commit_versions, 1 and 2; and a timestamp. Now let's run an UPDATE statement, SET IsEnabled to 0 for Id 2, and run this. And let's run a DELETE statement. This will delete the record for Id 1. Once again, let's get the changes using table_changes. Notice this other option, we are passing starting version as 3 and ending version as 4, so this will show changes by version 3 and version 4. Let's run this. See the feed for UPDATE operation, update_preimage is the record before update and update_postimage has the values after update. Finally, there is a record for delete operation. So change data feed is extremely powerful that can help to perform incremental loads.

Delta Architecture: With Batch Data & Change Data Feed
Now that you have seen Delta architecture and change data feed, let's implement this architecture for batch data using change feed. Now here, we'll create all the three layers. First, we'll take files and append raw data to bronze layer. Then, we'll use the change feed of bronze layer and perform incremental load into silver layer. This is where we'll perform a merge operation. And finally, we'll re compute the whole gold layer with each run, so this will be an overwrite operation. Remember, this is just to show you the capabilities, but you can change it according to your requirements. Back to Databricks workspace, let's use a new notebook to implement this. First, let's create a table, YellowTaxis_Bronze to store raw data. Here, let's keep all the columns we are going to receive from source. Additionally, let's add FileName column. It will store the name of file from which the record will come and a CreatedOn column to store the timestamp when the record will be added to the table. Let's partition this table by VendorId and enableChangeDataFeed on the table. Let's run this. Next, let's create a Silver table. Let's keep only limited columns that we need. Let's define some generated columns like PickupYear, Month, and Day and then two timestamps to track the records. Notice, this table is partitioned on a different column, PickupLocationId. Let's run this. And finally, let's create a Gold table to store aggregations. Here, we'll store TotalRides, Distance, and Amount. Let's create this, and all the tables are now ready. Now let's insert raw data into Bronze table. This can be done using the COPY command we used in the earlier modules. So here, let's point to a folder. Currently, this folder only has one file for YellowTaxis. Define the columns and their data types, and use these functions to get FileName and the CURRENT_TIMESTAMP. Let's run this. And you can see, this has loaded 10,000,000 million records into Bronze table. Let's check the table history. The COPY operation was version 1, so let's use this version to get change data feed. You can use table_changes function to get change data feed. Pass in the Bronze table name and version as 1. Let's execute this. And there you can see all 10,000,000 records showing up as inserts. Sounds good. So let's create a temporary view called BronzeChanges, and let's capture the changes from Bronze table using the same query. Additionally, you can apply filter conditions to clean up the data or even perform transformations if you want. Let's create this. Now let's use data in BronzeChanges and merge it to Silver table. For that, let's write a MERGE statement. The Silver table is the target table, and BronzeChanges is the src table. This means only the changes in Bronze table will now be merged into Silver table, right? Now define the UPDATE statement or records present in Silver table and INSERT statement if it's a new record. Let's execute this. And you can see, with the first operation, records have been pushed to Silver table. Now let's write an aggregation query on Silver table, group by certain columns, and aggregate the number of rides, total trip distance, and total amount. And then, let's overwrite the whole Gold table. For that, you can use INSERT OVERWRITE command. If you're using Python, you can use overwrite mode within DataFrame. Of course, you can perform incremental update to Gold. How? Capture changes from Silver table, and then write a MERGE statement. All right, let's run this, and this overwrites the Gold table. Run SELECT queries on all the tables and see the data. Now let's see how incremental update will work. In Azure portal, let's drop another file in the same folder. Think of this like a file dropped on the next day with additional changes. Now let's run the COPY command again. Will it pick both the files? Remember, COPY operation keeps track of files it has loaded, so only the new file will be pegged. Let's run this, and this has inserted only four records into Bronze table. Let's check the Bronze history, and you can see the latest commit version is 2. Once again, recreate the temporary view, but this time GetChanges started from version 2, and run this. Now let's push the changes to Silver table and run the MERGE statement. Notice, one record has been updated, and three new records have been inserted. So now we have an incremental update to Silver table. And finally, let's override the whole Gold table by running an aggregate query on Silver. So that's how you can work on creating Delta architecture, and change data feed can play a very important role in this.

Summary
In this module, you saw what is Lakehouse architecture. It intends to bring the best of data warehouses and Data Lakes together. You also saw its various objectives, like it should handle all types of data, provide ACID guarantees, ETL activity should be reduced, it should use open file formats, etc. And then you saw its different layers. There is storage layer, where you can use Data Lake, the transactional layer can be built using Delta Lake, for querying, there is Delta Engine, and for security and governance, there is Azure Active Directory and Databricks Unity Catalog. Then to handle the data inside Lakehouse, there is Delta architecture. It intends to improve the quality of data and serve different personas. You can build multiple layers, like Bronze, Silver, and Gold. Next, you saw another feature of Delta Lake, the Change Data Feed. You can enable it at the table level, and once enabled, changes are recorded for each transaction. Now to read the changes, you can provide the version from where you want to extract the changes. Finally, you saw how to implement Delta architecture and build Bronze, Silver, and Gold tables. And this was done for batch data, using Change Data Feed. Similarly, this can be built for streaming data as well. Now, in the next module, let's look at how to build ETL pipelines with Delta Live Tables.

Building ETL Pipelines with Delta Live Tables
Module Overview
Hey, everyone! Welcome to this module on Building ETL pipelines with Delta Live Tables. So first, we'll understand what are Delta Live Tables and its components. Then we'll see how to build a simple, but an end‑to‑end ETL pipeline using this product so we can understand its components. And finally, we'll implement Delta architecture using a complex and incremental pipeline that will handle multiple entities.

Understanding Delta Live Tables
Let's first understand the challenges to build ETL pipelines, and then, how Delta Live Tables can help us overcome that. In the previous module, we discussed the Delta architecture. It can have different layers like Bronze, Silver, and Gold. The Bronze Layer is the place where you can keep raw data. This data can then be cleaned, filtered, and transformed, and stored into Silver Layer. And then there is Gold Layer, where aggregated, business‑specific data can be stored. Now while this looks pretty good, but real‑life projects are not so straightforward. There are always multiple entities to deal with, like customers, sales, products, etc. So you will have multiple entities holding raw data and they will have different data quality issues. Then, based on them, there will be multiple tables in Silver Layer, and one bronze table might feed multiple silver tables. Then, depending on business requirements, there will be gold tables, and data in them might be based on joins on silver tables. Make sense? So building and maintaining these extract, transform, and load, or ETL pipelines, can become quite complex. But it's not just the data movement that brings complexity, there are several other factors too. Let's look at them. One of the biggest ones is managing the infrastructure and its scalability. We might need to handle batch and streaming data together. This is an important objective of Lakehouse. Then, there should be a robust mechanism to handle failures, and retry loading if possible. Next, we need to monitor, optimize, and maintain these entities, but along with this, it's also important to ensure that data remains of high quality, right? Now to have clarity, it's important to maintain dependencies between entities and track their lineage to perform troubleshooting, like how the data is flowing. And finally, pipelines need to be deployed in different environments, like dev, production, etc. So you can see, there are several challenges in building complex ETL pipelines. And this is where Delta Live Tables come in. Delta Live Tables is an ETL framework, specifically for Delta Lake Tables. It can help to build reliable, automated, testable, and declarative ETL pipelines. So it can address the challenges that we just discussed. So, what kind of pipelines can be built? You can declaratively build these kind of pipelines with extremely less effort, and we'll build the same pipeline in this module, using Delta Live Tables. But before that, let's quickly talk about the components of Delta Live Tables. First, there are datasets. They are basically the Delta Tables. Then, there are data quality checks. Next, there are queries to define business logic. There are pipelines, that you just saw. And finally, we have environments. You'll see all these components in this module. So let's see the steps to create ETL pipelines. First, you need to create live datasets. These are Delta tables or views, but are referred as live datasets. There are three types ‑ Complete, Incremental, and Streaming datasets. Next, you need to define data quality checks. These are also called as expectations. You can define the constraints you want to apply on data, and then the actions, if data does not satisfy the constraints. Then, you need to define queries to perform transformations. This is basically your business logic to clean, filter, transform, and aggregate the data. In fact, you can join multiple tables, and DLT will automatically identify the interdependencies between datasets. And then, you can create pipelines to execute your queries. The pipelines are extremely powerful. They can automatically manage the infra, process the data, and maintain lineage between entities. Also, it can apply data quality checks, handle logs, failures, etc. Now there are two modes to execute pipelines ‑ Triggered mode, and Continuous mode. And finally, you can move your pipelines from development to production. Sounds good? Let's see how to build pipelines using these steps.

Building Simple ETL Pipeline with Delta Live Tables
Now let's see how to build a simple ETL pipeline with DLT, or Delta Live Tables. So let's implement Delta architecture for YellowTaxis. We'll create one Bronze, one Silver, and two Gold tables here. This is exactly the same implementation as we did in the previous module, but this time it will be using DLT. This will help us understand the different components of DLT. Back to Databricks workspace, I'm using a new notebook, DLT YellowTaxis. First, let's create a Bronze table. Notice it's a live table. Is it different from a regular Delta table? No, it's not. Live means it will be managed by DLT. Let's name it as YellowTaxis_BronzeLive. Define the columns. Let's define the table format as Delta. Define its location partitioned by VendorId and provide a description. Now for a live table, it's mandatory to define the SELECT statement. This means how the data will be populated into this table. So let's read YellowTaxisParquet folder from Data Lake. Currently, there is only one file in this folder. If you're following along, make sure to check that it only has one file, YellowTaxis1.parquet since Bronze tables keep raw data, so we're not adding any transformations. Let's execute this. Notice, it is only checking for the syntax, but since it's a live table, this command can only run via pipelines. Sounds good. Next, let's create a Silver table, YellowTaxis_SilverLive. This is also a live table, and you can define this like a regular Delta table. Notice, it has less columns, and it has some generated columns as well, PickupYear, PickupMonth, and PickupDay. But here is the interesting part. You can define data quality checks or expectations. Let's define a CONSTRAINT, Valid_TotalAmount. The expectation is that TotalAmount should not be null, and it should be greater than 0. If any row violates this, you can define an action, DROP ROW. This will simply drop the row. In the same way, let's define one for TripDistance and define one for RideId. Notice, if it is null or less than or equal to 0, it will fail the process. There are two actions available at the time of recording this course, DROP ROW and FAIL UPDATE, but other actions will be released soon. Now define the other properties. This table has a different location and a different partition key. Once again, define the SELECT statement. Now here, let's select from the Bronze table. Notice, to access the live table, you have to use the live keyword. So this statement will simply select a few columns from Bronze and add it to Silver table. In the same way, let's create a live Gold table, SummaryByLocation. Notice, we are not defining any columns here. It will take the schema from the query. Let's refer to live Silver table and aggregate the data by pickup and drop locations. Finally, let's create one more Gold table, SummaryByDate. It is again referring to live Silver table, and it's aggregating the data by PickupYear, Month, and Day. That's it. The datasets along with expectations and transformations are now set. Easy, right? Now from the left menu, go to the Jobs tab, and there you can see an option, Delta Live Tables. Go to that, and let's create a new pipeline. Now there are three different editions, Core, Pro, and Advanced. You can write your ETL code in Python or SQL, incrementally add the data to live tables, and all of them have support for autoscaling. Then, the CDC option is basically the mod support only available in Pro and Advanced. And finally, Advanced edition has support for constraints and Photon engine. Notice, all of them maintain log information up to certain days. Now since we are going to use constraints, let's select the Advanced edition. Now add a name for the pipeline. And then in Notebook Libraries, click on Select, and select the notebook, Building YellowTaxis. All right, next you can define a target database, let's say TaxisDB, so all the live tables will be created here. This is useful because the database name is not getting hard coded in the notebook. You can also define the storage location of Data Lake to keep the files. Let's add the mount point and the folder part. Now define how you want to run your pipeline. Triggered mode means cluster will start, execute the pipeline, and shut down. And in continuous mode, it will keep running and executing the pipeline. This is useful in case of streaming jobs. And finally, define the infrastructure. See, how simple is this? You can enable or disable autoscaling and define the number of workers. Let's keep it as 1. Now along with this, you can even enable Photon engine. So this is all you have to do to set up the pipeline. Easy, right? Let's create it now, and the pipeline is now ready. Since it's a triggered mode, you can define a schedule, or you can run it immediately. Let's click on Start, and you can see it's setting up the infra. This might take a few minutes for the first time. Then, it is looking at the code and creating Delta tables. And there you can see it has identified the dependencies between datasets. There is one Bronze, one Silver, and two Gold tables, and it is executing now. You can see the logs here. Now let's click on Bronze table. It shows the schema and see the data quality. 5 less than 10,000,000 records have been written to Bronze. Let's click on Silver table. Notice the number of written and dropped records. By dropped, this is because of the constraints we created on this table. See the number of records dropped because of each constraint. And then there are two Gold tables, one with 35 and other with 22K records. Now I'm sure you are acknowledging how simple this process was. Easy declaration of ETL and data quality checks, portal setup of infra and Delta tables, finding dependencies between tables, and loading the write records. That's how Data Live Tables work. And remember, this is just the start. Now if you remember, we used the syntax, CreateLiveTable. This is called Completed table mode. This means if you run the pipeline again, all data in the tables will be overwritten. Make sense? Now in the Azure portal, let's go to YellowTaxisParquet folder, and lets upload another file, YellowTaxis2.parquet. This file only contains four records. Now let's start the pipeline again. Give it a few minutes to complete. Let's click on Bronze table. Notice, it has picked up both the files. This means it's reprocessing the first file again. Now since all tables are Completed mode tables, data in all of them will be overwritten. And finally, you can switch from Development to Production mode. Let's see the differences between them. There are two execution modes. If you're using Development mode, it reuses the same cluster for multiple pipeline runs. This avoids overhead of starting a cluster. But if the pipeline fails, DLT does not try to run it again. On the other hand, there is Production mode. Here, a new cluster is created for each run, and it shuts down as soon as the run is complete. Also, if there are any transient issues like network connectivity, it automatically tries to run it again. Sounds good. Now can we query the tables? Yes, of course. Let's use a new notebook and describe the Bronze table. Let's run this. Now the first operation was DLT SETUP. This created the table and connected to a pipeline. You can see that in the operation parameters. And then, there are two WRITE operations because we've ran the pipeline twice. Now you can run any operations on this table just like you would do on a regular Delta table. Run some queries and see for yourself. So let's summarize what we learned here. You can create multiple live tables using live keyword. And for these tables, you can define data quality checks by specifying constraints you want to apply and the action in case of an error. You saw two options, drop row and fail update. Other options will be available soon. Next, you can write your business logic in queries, and this can be used to clean, filter, transform, and aggregate the data. You can refer to other datasets in queries, and this helps DLT to build the lineage. Once ETL code is ready, you can create and run the ETL pipelines, which will take care of infra, the lineage, apply checks, process data, and do much more. And you also saw two execution options, triggered and continuous. And finally, you can promote your pipeline from dev to production.

Building Complex & Incremental ETL Pipeline
Now that we have built a pipeline that fully reloads every time, let's see how to build a complex and incremental ETL pipeline. There are two table types we'll look into, Complete table, and Incremental table. Once again, we'll implement a Delta architecture, but this time with two entities ‑ Yellow Taxis and Taxi Zones. Taxi Zones is a small table, so we'll load it fully every time in Bronze Layer. This is called Complete table in DLT. And then create a view in Silver Layer. Next, in the Bronze Layer, Yellow Taxis will be loaded incrementally. This means only the new files from source will be loaded to Bronze table. This is called Incremental Table in DLT. If you want to wrap any complex logic, you can also create an incremental view on top of Bronze table. And from the Bronze view, Silver table for Yellow Taxis will be loaded. The Silver table will also be an incremental table since only new data from Bronze will be loaded here. Make sense? And finally, there will be an aggregated table in Gold Layer based on Yellow Taxis. This will reload every time, so this will be a complete table. And we'll create another complete table that will reload based on a join between Taxi Zones and Yellow Taxis. So you can see, depending on requirements, ETL pipelines can become quite complex. Now before we create the pipeline, let's see what functionality is supported in DLT. If you're reading from Complete table and writing to another Complete table, data is fully overwritten. This is what you will see with Taxi Zones. Then, if you're trying to load from Complete to Incremental table, this is not possible. Next, reading from Incremental and loading to Complete table is a full rewrite, and data is overwritten. You will see this flow for Yellow Taxis Silver to Gold, And finally, if you're loading from one Incremental to another Incremental table, it's only the movement of new records. We'll do this for Yellow Taxis Bronze to Silver. Take a note of this, you will see all these scenarios. Back to the workspace, let's create a new notebook, DLT TaxiZones. First, let's define a Bronze table. Notice, CREATE LIVE TABLE means it's a complete table. Provide the other properties, like SCHEMA, LOCATION, COMMENT, etc. By default, it'll be a delta table. Then, define the SELECT statement, let's point to a folder, TaxiZones. Since it's a complete table, it will reload every time. Next, since data is very simple, let's create a view in Silver Layer. Again, it's a complete view. You can even define constraints on a view, like we did for the table, and then define the SELECT statement. Notice, the Bronze table is being referred using Live keyword. Sounds good? Now let's create another notebook, DLT YellowTaxis Incremental. Create a Bronze Ttable for YellowTaxis. Notice the syntax. This is an INCREMENTAL LIVE TABLE. Define the columns, like we did previously, define the properties, and then define the SELECT statement. This time, we are selecting from cloud_files. What's that? This is a tool in Databricks called Autoloader. This tool can pick up files from storage incrementally, so that'll help us to incrementally load the data into table. So in the method, pass the folder path. Currently, there is only one file present in this location. If you're following along, please verify that. Then, define the format as CSV, and then define a property to automatically infer the schema. You can then select columns from file and cast them to write data types, and this defines the Live Bronze table. As you saw in the diagram, you can also create an incremental view, Bronze Live Incremental View. Remember, this view will only receive new records from Bronze table. Here also, you can define constraints and a SELECT clause. Now to pick up data from Bronze table incrementally, you have to use STREAM function and pass in the table name. Now let's create an incremental Silver table for YellowTaxis. Define the columns and properties, but where is the SELECT statement? If you're building an incremental table, there are two options. One, if you only want to append the new data, you should define the SELECT statement; and second, if you want to merge the data, do not define SELECT statement. Since we are going to merge the data, let's not define SELECT here. Make sense? And now let's define the command to merge the data. In DLT, there is no merge statement, you have to use APPLY CHANGES command. So apply the changes into Silver table by taking data from Bronze view. Since we are going to pick up data incrementally from Bronze, it must be called using STREAM function. Then, define the keys to join, let's join on RideId and VendorId. The same keys must be present in both the datasets. And finally, Bronze view has a column called CreatedOn, let's sort the Bronze table data by this column, so all changes are applied in sequence. You can see DLT has an entirely new syntax, very different from regular delta tables. It will take some time to get used to it. And finally, let's create a Gold table. Notice, it's a complete table, and for this, let's define an aggregation query, but this time, since we want to pick up all data from Silver, do not use the STREAM keyword. And let's define another Gold table, SummaryByZone, but this time, let's join YellowTaxis and TaxiZones, and perform the aggregations. This will recompute the whole table again. Now let's revisit the YellowTaxis code again. First, we are creating an incremental Bronze table. With every run, only the new files will be picked up by Autoloader and loaded into Bronze table. Next, we are creating an incremental view. This view is referring to Bronze table and will only pick up changes in Bronze table, and we can get the changes from incremental table by using STREAM function. Next, we are creating Silver table, but here, we haven't provided SELECT statement. This is because we are going to merge changes into this table. And then we are defining the APPLY CHANGES statement. This is going to take the data from Bronze view and merge the changes in the Silver table. And finally, we are creating two complete Gold tables, one directly on Silver table by aggregating the data, and second, by joining YellowTaxis and TaxiZones, and aggregating data by zone. Sounds good? All right, now let's create a pipeline. Go to Jobs tab and select Delta Live Tables. Let's click on Create Pipeline, add the name for pipeline, DLT Incremental Pipeline. Now since we have two notebooks to run, let's select both of them, TaxiZones and YellowTaxis incremental. Once again, define the target database, TaxisDB, and the mounted part as the storage location. Disable the autoscaling, and select Workers as 1. That's it, let's create the pipeline. Make sure that YellowTaxis folder only has one file, and now, let's start the pipeline, and if everything is correctly set, it will show the chart of dependencies. Interesting, right? Click on Bronze table for YellowTaxis. You can see it has loaded close to 10 million records, and these records have gone to Silver table. Also, TaxiZones have loaded 263 records. Now, go to Azure portal, and lets upload one more YellowTaxis file, YellowTaxis2.csv. This file only contains four records. Now in the workspace, let's run the pipeline again, and see, this time, only four records have been picked up, and this is coming from the new file. This is because it's an incremental table. Further, these changes are getting merged into Silver table, but TaxiZones table has been reloaded since it's a complete table. So notice, TaxiZones' flow is reading Complete table and loading to Complete table. This is a reload every time. YellowTaxis Bronze to Silver is Incremental to Incremental load. This will only load new changes. And YellowTaxis Silver to Gold is an Incremental to Complete load. So this will also reload every time. Of course, there can be many possible scenarios, but I hope you got the idea how to build complex ETL pipelines with Delta Live Tables.

Summary
In this module, you saw what are Delta Live Tables. It's a framework to build reliable, automated, testable, and declarative ETL pipelines. Then you saw the different components of DLT and what's their purpose. First, there are live datasets. These are Delta tables or views, but are referred using live keyword, and they can be of complete or incremental type. Then you can filter the data on the table by applying data quality checks. For that, you need to define constraints and actions. Currently there are Drop Row and Fail Update actions. Then you saw queries, where you can define your business logic. This logic can append or merge the data to the table. In fact, you can join multiple datasets, and DLT automatically builds the lineage for that. Next, there are pipelines. As you saw, pipelines can do many things. It can handle the infra, maintain lineage, apply data quality checks, process data, and do much more, and you can run them in triggered or continuous mode. And finally, you can run your pipelines in development or production environments. So using that, you saw how to build end‑to‑end, complete and incremental ETL pipelines. Now, in the next module, let's look at how to implement some common use cases.

Implementing Common Use Cases
Module Overview
Hey everyone. Welcome to this module on Implementing Common Use Cases. So here, let's see how to query Delta Lake with Azure Synapse Analytics, and then let's see how to connect and consume the Delta tables in a visualization tool like Power BI. Of course, there can be many use cases. Feel free to drop a message in the discussion board if you're looking for any particular use case.

Querying Delta Tables with Azure Synapse Analytics
Data Lake is now supported by many compute engines. So let's see an example how Delta tables can be queried using Azure Synapse Analytics. Before we look at how to query the data, here is a quick introduction. Azure Synapse Analytics is a Unified Analytics Service. You can use it to perform data ingestion, data warehousing, big data analytics, and much more. Now multiple compute engines are built into Synapse. It has dedicated SQL pool, Apache Spark pool, Serverless SQL pool, etc. Now, if you want to get started with Synapse or learn more about it, you can check out my other courses. There is a Data Literacy course, covering Essentials of Azure Synapse, and a deep dive course, Building Your First Data Lakehouse Using Azure Synapse Analytics. Now let's see how to query Delta Lake in Synapse. You can use Apache Spark pool. Synapse Spark pool has support for open‑source Delta Lake. Remember, all features of Delta Lake that you saw are not available here. This is because Delta Engine, Data Live Tables, etc. are only available in Databricks. Now, in order to use it, you need to set up Spark pool, and then you can read and write the data in Delta format. Sound good? Now you can also query Delta using Serverless SQL pool. Serverless SQL pool does not require any pre‑provision infrastructure, you can directly query the Delta format, but currently, you cannot use it to write the data in data format, and you only pay for the amount of data used by your queries. So let's see this in action. To follow along, you will need to set up Azure Synapse workspace, and it should be connected to the storage account where you have stored the Delta Lake files. To set up Synapse, you can check out the Setup Instructions document available in the Exercise files section of the course. Now, in this demo, we'll query Delta Lake using Synapse Spark pool and also using Serverless SQL pool. Now, I am inside Synapse workspace. Let's first create a Spark pool. Go to Manage tab, let's select Apache Spark pools, and create a new one. Provide a name, let's say, sparkpool, select the Node size as Small, disable the Autoscaling, and select Number of nodes as 3. Now go to Additional settings, select an Apache Spark version, and there you can see the Delta Lake component will also be installed on the Spark pool, so Synapse has built‑in support for Delta Lake. Click on Review + create and create the pool, and the pool is now ready. Now from the left menu, go to Data tab, go to Linked section, from here, expand the Data Lake account, and select the taxidata container. You can now see the built‑in Data Explorer. Let's navigate to the Output folder, and there we have the YellowTaxis.delta folder. Open the folder and open one of the partition folders. Now to query Delta Lake folder, let's use a shortcut. From here, you can right‑click on any file, select New notebook, and Load to DataFrame. This opens up a new notebook to work with. Attach the notebook to sparkpool. Now change the path to only point to YellowTaxis.delta and change the format to Delta. That's it. Now click on Run cell. This will start the Spark pool and run the query, and now you can see the output from Delta format. In the same way, you can run almost all the Delta Lake commands here that you saw in Databricks. Sounds Good? I am leaving few commands on the screen for you to try it out. Once done, click on this icon of Stop session. Now let's see how to query Delta using Serverless SQL pool. Again, let's go to Data Explorer, right‑click on the Parquet file, this time, select New SQL script, and select Top 100 rows. This opens up a SQL script. Notice, this SQL script is connected to Built‑in engine. This is this Serverless SQL pool. Once again, change the path to only point to YellowTaxis.delta folder, and change the FORMAT to DELTA, click on Run to run the SQL query, and there you can see the output. So without provisioning any infra, you can run queries on Data Lake, and you only pay for the amount of data processed by this query. Interesting, right? So this clip was just to show you how multiple compute engines are now supporting Delta Lake, and I'm sure you got the idea.

Consuming Delta Tables in Power BI
Finally, let's see how Delta tables can be consumed in Power BI. Power BI is a visualization tool, and it can connect to Delta tables using a Spark cluster. This clip is just to demonstrate how visualization tools can work with Delta. To follow along, you will need to have Power BI installed on your machine. The link to download it is available in this setup instructions document. Now we'll see two options. First, let's connect using cluster information. For that, we'll need to copy the server name and HTTP path from the cluster configuration. Next, we'll need a Databricks access token, and then we can use these credentials in Power BI. Let's see how to do that. Back to Databricks workspace, go to the Compute tab, and let's open the cluster through which we want to connect. Let's click on DemoCluster, scroll down, and it has Advanced options. Here, click on JDBC/ODBC, and from here, let's copy Server Hostname and HTTP Path. Now from the left menu click on Settings, and then select User Settings. Here, we can generate access tokens. Let's click on Generate new token and click Generate, and this generates a token. Let's copy this one. Now let's open Power BI, click on Get data to connect to the Databricks cluster. Let's search for Databricks, and there you can see the Databricks connector. Now let's paste the information we just copied, server name and HTTP path, and click OK. Now it is asking for credentials. You can either sign in using Azure AD credentials or use Access Token. Let's add the access token and connect to it. There you can see all databases, and we have our database, taxisdb. Let's select yellowtaxis, and now you can load the data. Drag and drop the attributes on the report, and start working with it. Easy, right? Now let's see the second option. If you are using the Premium SKU of Databricks, you can connect to Power BI using Partner Connect. Partner Connect has connectors to connect to various tools, so in the workspace you can open Partner Connect and select Power BI. You can simply download the connection file from here and then open the file in Power BI, pass in the credentials, and start working with it. Let's see how to do that. Back to the workspace, on the left menu, you will see an option of Partner Connect. This is only available in Premium SKU. Click on that, and this shows various tools which can quickly connect to Databricks. Let's scroll down, and there you can see Power BI. You can even connect to other tools like Tableau. Let's select Power BI. Now it's asking for the cluster through which we would like to connect. Let's select DemoCluster, and let's download the connection file. Now open the connection file, and this opens up in Power BI. Now it's asking for credentials. Once again, you can log in using Azure AD credentials or use Access Token. Let's select Azure AD this time, and sign into it. Once signed in, it's showing a list of databases we can work with, and you know the process from here, right? So that's how you can access Delta tables in Power BI or other visualization tools.

Course Summary
Let's summarize what we have covered in this course. So Delta Lake is an open source storage layer that brings reliability to Data Lakes. Along with data, it keeps transaction logs, and because of which it is able to provide ACID guarantees. Then you saw its various features. It allows you to run DML operations, handle schema, apply table constraints, perform time travel, etc. You can also store batch and streaming data together. Delta Lake can work both as a source and a sink, and it can seamlessly handle multiple readers and writers. Then you saw Delta Engine, which is a high‑performance query engine only available in Databricks. It has three components, multiple built‑in optimizations like data skipping, bin‑packing, Z‑ordering, auto optimization, etc. Then there is a new vectorized query engine, Photon. And there is Delta Cache. Remember, some optimizations have been made open source now. Next you saw how lakehouse intends to bring the best of data warehouses and Data Lakes together, and it can be built using the components you saw here. Then, Delta architecture is a part of lakehouse that focuses on storage part to improve data quality. And there is the ELT framework to build ETL pipelines. It can help you build declarative, automated, and reliable pipelines by taking care of infra, lineage, data quality, etc. And in this module, you saw several use cases, like connecting to Azure Synapse, Power BI, etc. But of course, there are many more we can add here. And this brings us to the end of this course. I hope you enjoyed it. Thanks for watching ,and continue learning.
![image](https://user-images.githubusercontent.com/24469318/208910961-ec5a2855-7301-474f-8b96-0df781633cab.png)

